= Module 1: User workload monitoring
:source-highlighter: rouge
:toc: macro
:toclevels: 1

Your manager at Red Hat Observability Inc. just assigned you to improve visibility into the company's microservices application. "We need to know when our applications are having problems before customers call us," she says. "Right now, we only monitor infrastructure, not application health."

Red Hat Observability Inc. needs custom application metrics and alerts that reflect business-critical functionality. You've been tasked with implementing user workload monitoring to track application performance, latency, and error rates.

In this module, you'll learn how to configure Prometheus monitoring for custom application metrics and create alerts that notify you when applications deviate from expected behavior.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand the 3 pillars of observability (metrics, logs, traces) and when to use each
* Configure ServiceMonitor resources to collect custom application metrics
* Write PromQL queries to analyze application performance
* Create custom dashboards in the OpenShift console for application visibility
* Configure Alertmanager rules to proactively notify you of application issues

== Understanding observability foundations

Before implementing monitoring, you need to understand the observability landscape at Red Hat Observability Inc.

=== The 3 pillars of observability

Comprehensive observability relies on 3 complementary signal types:

* **Metrics**: Numerical measurements over time (CPU usage, request count, latency percentiles)
  - Use for: Trends, thresholds, alerting on quantitative changes
  - Example: "API response time increased 200% in the last 10 minutes"

* **Logs**: Discrete event records with timestamps and context (error messages, transaction IDs, user actions)
  - Use for: Detailed context, root cause analysis, audit trails
  - Example: "User 12345 received error: database connection timeout"

* **Traces**: Request flows across distributed services (span duration, service dependencies, bottleneck identification)
  - Use for: Understanding microservice interactions, performance optimization
  - Example: "Order API → Inventory Service → Database: 850ms total, 700ms in database query"

**When to use each signal**:

* Start with **metrics** to detect that something is wrong (high error rate)
* Use **logs** to understand what went wrong (specific error messages)
* Apply **traces** to find where the problem occurred (which service caused the delay)

=== Observability methodologies

Beyond the 3 pillars, different methodologies help structure which metrics to collect:

**RED Method** (Request-driven services):

* **Rate**: Number of requests per second
* **Errors**: Number of failed requests per second
* **Duration**: Time taken to process requests (latency)

Best for: Microservices, APIs, web applications

Example: "Our checkout API has 500 req/s, 2% error rate, and p95 latency of 250ms"

**USE Method** (Resources):

* **Utilization**: Percentage of time the resource is busy
* **Saturation**: Degree of queued work (e.g., CPU run queue)
* **Errors**: Error count from the resource

Best for: Infrastructure components (CPU, memory, disk, network)

Example: "Database server has 85% CPU utilization, 10 connections queued, 0 disk errors"

**Golden Signals** (Google SRE):

* **Latency**: Time to service requests
* **Traffic**: Demand on your system
* **Errors**: Rate of failed requests
* **Saturation**: How "full" your service is

Best for: Comprehensive service monitoring combining request and resource metrics

NOTE: In this workshop, you'll primarily use the **RED method** for monitoring the sample application's HTTP requests (rate, errors, duration). The Prometheus metrics `http_requests_total` and `http_request_duration_seconds` directly support this methodology.

=== OpenShift monitoring architecture

OpenShift provides multiple monitoring approaches to serve different use cases:

**Platform monitoring** (managed by cluster admins):

* Monitors OpenShift infrastructure and control plane components
* Includes: etcd, API server, scheduler, controller manager
* Namespace: `openshift-monitoring`
* Scope: Cluster-wide infrastructure metrics

**User workload monitoring** (for your applications):

* Monitors custom applications and user-deployed services
* You configure what to monitor via ServiceMonitor resources
* Namespace: `openshift-user-workload-monitoring`
* Uses Prometheus, Thanos Querier, and Alertmanager
* Scope: Cluster-wide application metrics (any namespace)

**Cluster Observability Operator (COO)** (for team-scoped monitoring):

* Namespace-scoped monitoring with MonitoringStack custom resource
* Label-based ServiceMonitor discovery (e.g., `monitoring.rhobs/stack: observability-stack`)
* Namespace: `observability-demo` (or any namespace you deploy the stack to)
* Uses Prometheus, Thanos Querier, and Perses dashboards
* Scope: Team or application-specific metrics with custom visualizations

**When to use each**:

* Use **platform monitoring** for cluster infrastructure (managed by admins, you can't modify it)
* Use **user workload monitoring** for simple application metrics without custom dashboards
* Use **COO** when you need custom Perses dashboards, namespace isolation, or team-scoped monitoring

In this workshop, you'll use **both user workload monitoring** (Exercises 1-3) and **COO** (Exercise 4 onward) to understand the full range of monitoring options available in OpenShift.

== Exercise 1: Explore the monitoring stack

You need to verify that user workload monitoring is enabled and understand the components deployed in your cluster.

The observability stack was pre-configured via GitOps, so Prometheus and Alertmanager should already be running.

=== Steps

. Log into the OpenShift console at {openshift_cluster_console_url}
+
Use the credentials provided in your lab interface:
+
* Username: {openshift_cluster_admin_username}
* Password: {openshift_cluster_admin_password}

. Verify user workload monitoring pods are running:
+
[source,bash]
----
oc get pods -n openshift-user-workload-monitoring
----
+
.Expected output
[source,text]
----
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-xxxxx              2/2     Running   0          1h
prometheus-user-workload-0             6/6     Running   0          1h
prometheus-user-workload-1             6/6     Running   0          1h
thanos-ruler-user-workload-0           3/3     Running   0          1h
thanos-ruler-user-workload-1           3/3     Running   0          1h
----

. Check the ConfigMap that enables user workload monitoring:
+
[source,bash]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring -o yaml
----
+
Look for `enableUserWorkload: true` in the configuration.

. Access the OpenShift console monitoring interface:
+
* Navigate to **Observe** → **Metrics** in the left navigation
* This opens the Prometheus query interface

. Run your first PromQL query to see cluster metrics:
+
In the query box, enter:
+
[source,promql]
----
up{namespace="openshift-user-workload-monitoring"}
----
+
Click **Run Queries**
+
This shows all targets being scraped in the user workload monitoring namespace. Each target should show `value: 1` (up and healthy).

=== Verify

Check that your monitoring stack is operational:

* ✓ All pods in `openshift-user-workload-monitoring` are Running
* ✓ User workload monitoring is enabled (`enableUserWorkload: true`)
* ✓ Prometheus query interface is accessible
* ✓ PromQL query returns results

**What you learned**: OpenShift provides 2 monitoring stacks (platform and user workload). User workload monitoring is where you'll configure custom application metrics.

=== Troubleshooting

**Issue**: No pods in `openshift-user-workload-monitoring` namespace

**Solution**: User workload monitoring may not be enabled. Check the cluster-monitoring-config ConfigMap and verify `enableUserWorkload: true` exists. If not, contact your cluster administrator.

**Issue**: PromQL query returns no data

**Solution**: Allow a few minutes for Prometheus to scrape targets. Check that the namespace exists: `oc get ns openshift-user-workload-monitoring`

== Exercise 2: Deploy a sample application with metrics

To practice monitoring, you need an application that exposes metrics. You'll deploy a sample application that provides Prometheus-compatible metrics.

NOTE: Your workshop namespace has been pre-created with special permissions to create ServiceMonitor and PrometheusRule resources. These permissions are required for this workshop and are granted via a custom ClusterRole.

=== Steps

. Create a new project for your sample application:
+
[source,bash]
----
oc new-project {user}-observability-demo
----
+
NOTE: If the namespace already exists, you can switch to it with `oc project {user}-observability-demo`. The namespace has been pre-configured with the necessary monitoring permissions.

. Deploy a sample application that exposes metrics:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: app
        image: quay.io/brancz/prometheus-example-app:v0.3.0
        ports:
        - containerPort: 8080
          name: http
      - name: debug
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command: ["/bin/sh", "-c", "while true; do sleep 30; done"]
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"
---
apiVersion: v1
kind: Service
metadata:
  name: sample-app
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  selector:
    app: sample-app
  ports:
  - port: 8080
    targetPort: 8080
    name: http
EOF
----
+
This creates a deployment with 2 replicas and a service to expose the application.
+
NOTE: The deployment includes a debug sidecar container with UBI minimal. This sidecar provides debugging tools like `curl` without bloating the main application container. This is a common pattern for production debugging.

. Verify the pods are running:
+
[source,bash]
----
oc get pods -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                          READY   STATUS    RESTARTS   AGE
sample-app-xxxxx-xxxxx        2/2     Running   0          30s
sample-app-xxxxx-xxxxx        2/2     Running   0          30s
----
+
Note the `2/2` ready status indicates both the application and debug sidecar containers are running.

. Check what metrics the application exposes using the debug sidecar:
+
First, make a request to the application to initialize the metrics:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080
----
+
.Expected output
[source,text]
----
Hello from example application.
----
+
Now check the metrics endpoint:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080/metrics | head -20
----
+
The `-c debug` flag specifies which container to exec into (the debug sidecar). Since containers in the same pod share the network namespace, the sidecar can access `localhost:8080`.
+
.Sample output
[source,text]
----
# HELP http_request_duration_seconds Duration of all HTTP requests
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="found",method="get",le="0.005"} 1
http_request_duration_seconds_bucket{code="200",handler="found",method="get",le="0.01"} 1
...
# HELP http_requests_total Count of all HTTP requests
# TYPE http_requests_total counter
http_requests_total{code="200",method="get"} 1
# HELP version Version information about this binary
# TYPE version gauge
version{version="v0.3.0"} 1
----
+
The application exposes several metrics:
+
* `http_requests_total`: Counter of all HTTP requests by status code and method
* `http_request_duration_seconds`: Histogram of request latency
* `version`: Application version information
+
NOTE: Metrics are lazy-initialized and only appear after the first request. This is why we made an initial request before checking the metrics endpoint.

=== Verify

Check that your sample application is ready for monitoring:

* ✓ 2 pods running in {user}-observability-demo namespace
* ✓ Service exposing port 8080
* ✓ Metrics endpoint returns Prometheus-formatted data
* ✓ Metrics include `http_requests_total` counter

== Exercise 3: Configure ServiceMonitor to collect metrics

Now you'll configure Prometheus to scrape metrics from your sample application using a ServiceMonitor resource.

A **ServiceMonitor** tells Prometheus which services to scrape for metrics. It uses label selectors to find services and defines scrape intervals and ports.

=== Steps

. Create a ServiceMonitor resource:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sample-app-monitor
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
EOF
----
+
This ServiceMonitor tells Prometheus to:
+
* Find services with label `app: sample-app`
* Scrape the `http` port
* Collect metrics every 30 seconds
* Use the `/metrics` path

. Verify the ServiceMonitor was created:
+
[source,bash]
----
oc get servicemonitor -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                 AGE
sample-app-monitor   10s
----

. Generate some traffic to create metrics:
+
[source,bash]
----
for i in {1..50}; do
  oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080 > /dev/null
  echo "Request $i completed"
done
----
+
This sends 50 HTTP requests to the application using the debug sidecar, which will increment the `http_requests_total` metric.

. Wait 1-2 minutes for Prometheus to scrape the metrics, then query them:
+
Go to **Observe** → **Metrics** in the OpenShift console.
+
Enter this PromQL query:
+
[source,promql]
----
http_requests_total{namespace="{user}-observability-demo"}
----
+
Click **Run Queries**
+
You should see results showing the counter values for each pod.

. Query the rate of requests over time:
+
[source,promql]
----
rate(http_requests_total{namespace="{user}-observability-demo"}[5m])
----
+
This shows requests per second averaged over 5 minutes.

=== Verify

Check that Prometheus is collecting your application metrics:

* ✓ ServiceMonitor exists in {user}-observability-demo namespace
* ✓ PromQL query `http_requests_total` returns data
* ✓ Multiple time series (one per pod) are visible
* ✓ Rate query shows request rate calculation

**What you learned**: ServiceMonitor resources configure Prometheus to scrape application metrics. Once configured, metrics are automatically collected and queryable via PromQL.

=== Troubleshooting

**Issue**: PromQL query returns no data

**Solution**: 
. Wait 1-2 minutes for Prometheus to scrape metrics
. Verify ServiceMonitor exists: `oc get servicemonitor -n {user}-observability-demo`
. Check that service labels match ServiceMonitor selector
. Verify pods are running and exposing metrics: `oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl localhost:8080/metrics`

**Issue**: Metrics show 0 requests

**Solution**: Generate traffic (run the curl loop again) and wait for Prometheus to scrape the updated values.

[[rbac-troubleshooting]]
**Issue**: Permission denied when creating ServiceMonitor

**Error message**:
[source,text]
----
servicemonitors.monitoring.coreos.com "sample-app-monitor" is forbidden: 
User "userX" cannot create resource "servicemonitors" in API group "monitoring.coreos.com"
----

**Solution**: This requires special permissions beyond the standard OpenShift roles.

**If using the GitOps deployment** (recommended):

The `make deploy` command creates:

1. A `workshop-user-monitoring` ClusterRole with ServiceMonitor/PrometheusRule permissions
2. RoleBindings in each user's namespace to grant these permissions

Verify the RoleBinding exists:

[source,bash]
----
oc get rolebinding workshop-user-monitoring -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                        ROLE                                    AGE
workshop-user-monitoring    ClusterRole/workshop-user-monitoring    10m
----

If missing, re-run:

[source,bash]
----
make deploy
----

**If deploying manually**:

A cluster administrator needs to grant monitoring permissions. The admin can either:

**Option 1**: Grant the pre-created monitoring ClusterRole:

[source,bash]
----
oc adm policy add-cluster-role-to-user workshop-user-monitoring {user} -n {user}-observability-demo
----

**Option 2**: Create and bind a custom ClusterRole:

[source,bash]
----
# Create ClusterRole (once)
cat <<EOF | oc apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: workshop-user-monitoring
rules:
- apiGroups: [monitoring.coreos.com]
  resources: [servicemonitors, prometheusrules, podmonitors]
  verbs: [create, delete, get, list, patch, update, watch]
EOF

# Grant to user
oc create rolebinding workshop-user-monitoring \
  --clusterrole=workshop-user-monitoring \
  --user={user} \
  -n {user}-observability-demo
----

== Exercise 4: Create custom dashboards with COO

Querying metrics via PromQL is powerful, but visualizing trends over time is more intuitive. You'll use the Cluster Observability Operator (COO) to create custom dashboards.

=== Understanding COO vs User Workload Monitoring

Your cluster has **two** monitoring systems:

**User Workload Monitoring** (Exercise 1-3):

* Cluster-wide monitoring managed by OpenShift
* Namespace: `openshift-user-workload-monitoring`
* Best for: Infrastructure monitoring, cluster-wide metrics
* You've already used this for your sample-app

**Cluster Observability Operator (COO)**:

* Namespace-scoped monitoring with MonitoringStack custom resource
* Namespace: `observability-demo`
* Best for: Application-specific monitoring, custom dashboards, team-scoped metrics
* Provides Perses dashboards for custom visualizations

**Key difference**: COO uses a **label selector** to discover ServiceMonitors. Your sample-app ServiceMonitor doesn't have this label, so it's only visible to user workload monitoring.

In this exercise, you'll create a **second** ServiceMonitor for COO with custom dashboards.

=== Steps

. Verify the COO MonitoringStack is deployed:
+
[source,bash]
----
oc get monitoringstack -n observability-demo
----
+
.Expected output
[source,text]
----
NAME                   AGE
observability-stack    1h
----

. Check the label selector the MonitoringStack uses to discover ServiceMonitors:
+
[source,bash]
----
oc get monitoringstack observability-stack -n observability-demo -o jsonpath='{.spec.resourceSelector.matchLabels}'
----
+
.Expected output
[source,json]
----
{"monitoring.rhobs/stack":"observability-stack"}
----
+
ServiceMonitors **must have this label** to be discovered by the COO stack.

. Create a **new** ServiceMonitor with the COO label:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sample-app-coo
  namespace: {user}-observability-demo
  labels:
    app: sample-app
    monitoring.rhobs/stack: observability-stack
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
EOF
----
+
Note the `monitoring.rhobs/stack: observability-stack` label - this tells COO to scrape this service.

. Verify the ServiceMonitor is picked up by checking Prometheus targets:
+
[source,bash]
----
oc get pods -n observability-demo -l app.kubernetes.io/name=prometheus
----
+
.Expected output
[source,text]
----
NAME                                READY   STATUS    RESTARTS   AGE
prometheus-observability-stack-0    2/2     Running   0          1h
prometheus-observability-stack-1    2/2     Running   0          1h
prometheus-observability-stack-2    2/2     Running   0          1h
----
+
Wait 1-2 minutes for Prometheus to discover the new ServiceMonitor.

. Create a Perses dashboard to visualize your metrics:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: perses.dev/v1alpha1
kind: PersesDashboard
metadata:
  name: sample-app-dashboard
  namespace: {user}-observability-demo
  labels:
    monitoring.rhobs/stack: observability-stack
spec:
  display:
    name: Sample Application Metrics
    description: HTTP request metrics for sample-app
  datasources:
    default:
      kind: PrometheusDatasource
      spec:
        directUrl: http://thanos-querier.observability-demo.svc.cluster.local:9090
  duration: 1h
  panels:
    HTTP_Request_Rate:
      kind: Panel
      spec:
        display:
          name: HTTP Request Rate
          description: Requests per second by pod
        queries:
          - kind: TimeSeriesQuery
            spec:
              datasource:
                kind: PrometheusDatasource
              query: |
                sum(rate(http_requests_total{namespace="{user}-observability-demo"}[5m])) by (pod)
        plugin:
          kind: TimeSeriesChart
          spec:
            legend:
              position: bottom
    Total_Requests:
      kind: Panel
      spec:
        display:
          name: Total HTTP Requests
          description: Cumulative request count by pod
        queries:
          - kind: TimeSeriesQuery
            spec:
              datasource:
                kind: PrometheusDatasource
              query: |
                sum(http_requests_total{namespace="{user}-observability-demo"}) by (pod)
        plugin:
          kind: TimeSeriesChart
          spec:
            legend:
              position: bottom
  layouts:
    - kind: Grid
      spec:
        display:
          title: Request Metrics
          collapse:
            open: true
        items:
          - x: 0
            y: 0
            width: 12
            height: 6
            content:
              \$ref: '#/spec/panels/HTTP_Request_Rate'
          - x: 0
            y: 6
            width: 12
            height: 6
            content:
              \$ref: '#/spec/panels/Total_Requests'
EOF
----
+
This creates a Perses dashboard with 2 panels: request rate and total requests.

. Generate traffic to populate the dashboard:
+
[source,bash]
----
for i in {1..100}; do
  oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080 > /dev/null
  sleep 0.1
done
----

. Access your custom dashboard in the OpenShift console:
+
* Navigate to **Observe** → **Dashboards**
* In the dashboard dropdown, select **Sample Application Metrics**
* You should see 2 panels showing request rate and total requests

. Verify the dashboard is using COO metrics:
+
The dashboard queries the Thanos Querier in the `observability-demo` namespace, which aggregates data from the COO MonitoringStack (not user workload monitoring).

=== Verify

Check that your COO dashboard is working:

* ✓ MonitoringStack exists in observability-demo namespace
* ✓ ServiceMonitor `sample-app-coo` has label `monitoring.rhobs/stack: observability-stack`
* ✓ PersesDashboard created successfully
* ✓ Dashboard appears in **Observe** → **Dashboards**
* ✓ Panels show request rate and total requests
* ✓ Graphs update with new data

**What you learned**: 

* COO provides namespace-scoped monitoring with label-based ServiceMonitor discovery
* Perses dashboards are declarative (YAML) and версioned with your application
* You can run multiple monitoring stacks: cluster-wide (user workload) + team-scoped (COO)
* COO dashboards use Thanos Querier for multi-replica query aggregation

=== Troubleshooting

**Issue**: ServiceMonitor not picked up by COO

**Solution**:
. Verify the label: `oc get servicemonitor sample-app-coo -n {user}-observability-demo --show-labels`
. Must have: `monitoring.rhobs/stack=observability-stack`
. Check MonitoringStack selector: `oc get monitoringstack observability-stack -n observability-demo -o yaml | grep -A5 resourceSelector`

**Issue**: Dashboard not visible in console

**Solution**:
. Verify PersesDashboard exists: `oc get persesdashboard -n {user}-observability-demo`
. Check for errors: `oc describe persesdashboard sample-app-dashboard -n {user}-observability-demo`
. Verify UI plugin is enabled: `oc get uiplugins.observability.openshift.io dashboards -o yaml`

**Issue**: Dashboard shows "No data"

**Solution**:
. Wait 2-3 minutes for Prometheus to scrape metrics
. Verify Thanos Querier is running: `oc get pods -n observability-demo -l app.kubernetes.io/name=thanos-querier`
. Generate traffic (run the curl loop again)
. Check Prometheus targets: Port-forward to COO Prometheus and check `/targets`

== Exercise 5: Configure alerting rules

Dashboards help you see current state, but alerts proactively notify you when problems occur. You'll create an alerting rule that fires when request rates drop unexpectedly.

=== Steps

. Create a PrometheusRule resource with an alerting rule:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sample-app-alerts
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  groups:
  - name: sample-app
    interval: 30s
    rules:
    - alert: LowRequestRate
      expr: sum(rate(http_requests_total{namespace="{user}-observability-demo"}[5m])) < 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Low request rate detected"
        description: "Application in namespace {{ \$labels.namespace }} is receiving fewer than 0.1 requests/second for more than 2 minutes."
EOF
----
+
This alert fires if request rate drops below 0.1 requests/second for more than 2 minutes.

. Verify the PrometheusRule was created:
+
[source,bash]
----
oc get prometheusrule -n {user}-observability-demo
----

. Check alert status in the console:
+
Navigate to **Observe** → **Alerting** in the OpenShift console.
+
* Click on the **Alerting rules** tab
* Search for "LowRequestRate"
* You should see the alert in **Inactive** or **Pending** state

. Trigger the alert by stopping traffic:
+
Wait 2-3 minutes without generating any traffic. The alert should transition from **Inactive** → **Pending** → **Firing**.

. Generate traffic to resolve the alert:
+
[source,bash]
----
for i in {1..200}; do
  oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080 > /dev/null
  sleep 0.5
done
----
+
After a few minutes, the alert should transition back to **Inactive**.

=== Verify

Check that your alerting rules are working:

* ✓ PrometheusRule exists in {user}-observability-demo namespace
* ✓ Alert appears in **Observe** → **Alerting** → **Alerting rules**
* ✓ Alert transitions through states: Inactive → Pending → Firing
* ✓ Alert resolves when traffic increases

**What you learned**: PrometheusRule resources define alerting conditions. Alerts transition through states (Inactive → Pending → Firing) based on PromQL expressions and duration thresholds.

=== Troubleshooting

**Issue**: Alert not visible in console

**Solution**:
. Verify PrometheusRule exists: `oc get prometheusrule -n {user}-observability-demo`
. Check syntax errors in PromQL expression
. Wait 1-2 minutes for Prometheus to reload configuration

**Issue**: Alert never fires

**Solution**:
. Verify condition is met (request rate < 0.1/sec)
. Check `for` duration (must be in low state for 2 minutes)
. Generate traffic then stop to test alert firing

== Learning outcomes

By completing this module, you should now understand:

* ✓ The 3 pillars of observability (metrics, logs, traces) and when to use each signal type
* ✓ OpenShift's dual monitoring architecture (platform vs user workload monitoring)
* ✓ Cluster Observability Operator (COO) for namespace-scoped monitoring with label-based discovery
* ✓ How ServiceMonitor resources configure Prometheus to scrape application metrics
* ✓ Writing PromQL queries to analyze application performance and calculate rates
* ✓ Creating declarative Perses dashboards for custom visualizations
* ✓ Configuring PrometheusRule resources to proactively alert on application issues

**Business impact for Red Hat Observability Inc.**:

You've implemented the foundation of proactive monitoring. Instead of discovering issues through customer complaints, you now have:

* Real-time visibility into application request rates and patterns
* Declarative dashboards versioned alongside your application code
* Team-scoped monitoring (COO) isolated from cluster infrastructure monitoring
* Automated alerts that notify you before problems impact customers

**Next steps**: Module 2 will add centralized logging with LokiStack, enabling you to correlate metrics with detailed log messages for faster root cause analysis.

== Module summary

You successfully demonstrated how user workload monitoring provides application visibility at Red Hat Observability Inc.

**What you accomplished**:

* Verified the user workload monitoring stack (Prometheus, Thanos, Alertmanager)
* Deployed a sample application with Prometheus-compatible metrics
* Configured ServiceMonitor to automatically collect application metrics
* Created namespace-scoped monitoring with Cluster Observability Operator (COO)
* Built declarative Perses dashboards to visualize application performance
* Implemented alerting rules to proactively detect application issues

**Key concepts mastered**:

* **ServiceMonitor**: Declaratively configures which services Prometheus scrapes
* **MonitoringStack**: Namespace-scoped monitoring with label-based resource discovery
* **PromQL**: Query language for analyzing time-series metrics data
* **PersesDashboard**: Declarative YAML-based dashboard as code
* **PrometheusRule**: Defines alerting conditions and notification thresholds
* **User workload monitoring vs COO**: Cluster-wide vs namespace-scoped monitoring

**Metrics collected**:

* `http_requests_total`: Counter of all HTTP requests by status code
* `up`: Target availability (1 = healthy, 0 = down)
* Custom application metrics from your deployed services

Continue to Module 2 to add centralized logging capabilities.
