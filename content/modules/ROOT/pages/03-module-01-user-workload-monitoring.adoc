= Module 1: User workload monitoring
:source-highlighter: rouge
:toc: macro
:toclevels: 1

Your organization needs to improve visibility into microservices application health and performance. The current infrastructure monitoring does not provide insight into application-level metrics or business-critical functionality.

You will implement user workload monitoring to track application performance, latency, and error rates. Custom metrics and alerts will enable proactive detection of application issues before they impact users.

In this module, you'll learn how to configure Prometheus monitoring for custom application metrics and create alerts that notify you when applications deviate from expected behavior.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand the 3 pillars of observability (metrics, logs, traces) and when to use each
* Configure ServiceMonitor resources to collect custom application metrics
* Write PromQL queries to analyze application performance
* Create declarative Perses dashboards for application visibility in the OpenShift console
* Configure Alertmanager rules to proactively notify you of application issues

== Understanding observability foundations

Before implementing monitoring, you need to understand the observability landscape.

=== The 3 pillars of observability

Use all 3 signal types together:

* **Metrics**: Numeric trends over time (request rate, latency percentiles, CPU)
  - Best for: Detection, thresholds, alerting
* **Logs**: Timestamped events with context (errors, transaction IDs, user actions)
  - Best for: Root cause details and audit trails
* **Traces**: End-to-end request flow across services
  - Best for: Finding latency bottlenecks and dependency issues

Practical workflow:

* Start with **metrics** to detect a problem
* Use **logs** to identify what failed
* Use **traces** to locate where it failed

=== Observability methodologies

Use these common methods to decide which metrics matter most:

* **RED** (services): **Rate, Errors, Duration**
* **USE** (infrastructure): **Utilization, Saturation, Errors**
* **Golden Signals**: **Latency, Traffic, Errors, Saturation**

NOTE: In this workshop, you'll primarily use the **RED method** for monitoring the sample application's HTTP requests (rate, errors, duration). The Prometheus metrics `http_requests_total` and `http_request_duration_seconds` directly support this methodology.

=== OpenShift monitoring architecture

OpenShift provides 3 complementary monitoring options:

* **Platform monitoring (CMO)**
  - Namespace: `openshift-monitoring`
  - Purpose: Cluster and control-plane health
  - Managed by: Cluster admins

* **User workload monitoring (CMO)**
  - Namespace: `openshift-user-workload-monitoring`
  - Purpose: Cluster-wide application monitoring using ServiceMonitor
  - Trade-off: Easy to use, limited customization

* **Cluster Observability Operator (COO)**
  - Purpose: Independent, namespace-scoped monitoring stacks via `MonitoringStack`
  - Discovery: Label-based selectors (for example, `monitoring.rhobs/stack: observability-stack`)
  - Components: Prometheus plus optional Thanos Querier, Alertmanager, and UI plugins (including Perses dashboards)
  - Best for: Multi-tenant teams, custom dashboards, and flexible retention/configuration

CMO and COO can run together without conflict.

In this workshop, you'll use **both user workload monitoring** (Exercises 1-3) and **COO** (Exercise 4 onward) to understand the full range of monitoring options available in OpenShift.

== Exercise 1: Explore the monitoring stack

You need to verify that user workload monitoring is enabled and understand the components deployed in your cluster.

The observability stack was pre-configured via GitOps, so Prometheus and Alertmanager should already be running.

=== Steps

. Log into the OpenShift console at {openshift_console_url}
+
Use the credentials provided in your lab interface:
+
* Username: {user}
* Password: {password}

. Verify user workload monitoring pods are running:
+
[source,bash]
----
oc get pods -n openshift-user-workload-monitoring
----
+
.Expected output
[source,text]
----
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-xxxxx              2/2     Running   0          1h
prometheus-user-workload-0             6/6     Running   0          1h
prometheus-user-workload-1             6/6     Running   0          1h
thanos-ruler-user-workload-0           3/3     Running   0          1h
thanos-ruler-user-workload-1           3/3     Running   0          1h
----

. Access the OpenShift console monitoring interface:
+
* Navigate to **Observe** → **Metrics** in the left navigation
* This opens the Prometheus query interface, make sure the namespace filter is set to `openshift-user-workload-monitoring`.

. Run your first PromQL query to see cluster metrics:
+
In the query box, enter:
+
[source,promql]
----
up{namespace="openshift-user-workload-monitoring"}
----
+
Click **Run Queries**
+
This shows all targets being scraped in the user workload monitoring namespace. Each target should show `value: 1` (up and healthy).

=== Verify

Check that your monitoring stack is operational:

* ✓ All pods in `openshift-user-workload-monitoring` are Running
* ✓ User workload monitoring is enabled (`enableUserWorkload: true`)
* ✓ Prometheus query interface is accessible
* ✓ PromQL query returns results

**What you learned**: OpenShift includes CMO-managed platform and user workload monitoring, plus optional COO stacks for advanced use cases. In this exercise, you verified the CMO user workload stack.

=== Troubleshooting

**Issue**: PromQL query returns no data

**Solution**: Allow a few minutes for Prometheus to scrape targets. Prometheus scrapes metrics every 30 seconds.

== Exercise 2: Deploy a sample application with metrics

To practice monitoring, you need an application that exposes metrics. You'll deploy a sample application that provides Prometheus-compatible metrics.

NOTE: Your workshop namespace has been pre-created with special permissions to create ServiceMonitor and PrometheusRule resources. These permissions are required for this workshop and are granted via a custom ClusterRole.

=== Steps

. Create a new project for your sample application:
+
[source,bash]
----
oc new-project {user}-observability-demo
----
+
NOTE: If the namespace already exists, you can switch to it with `oc project {user}-observability-demo`. The namespace has been pre-configured with the necessary monitoring permissions.

. Deploy a sample application that exposes metrics:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: app
        image: quay.io/brancz/prometheus-example-app:v0.3.0
        ports:
        - containerPort: 8080
          name: http
      - name: debug
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command: ["/bin/sh", "-c", "while true; do sleep 30; done"]
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"
---
apiVersion: v1
kind: Service
metadata:
  name: sample-app
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  selector:
    app: sample-app
  ports:
  - port: 8080
    targetPort: 8080
    name: http
EOF
----
+
This creates a deployment with 2 replicas and a service to expose the application.
+
NOTE: The deployment includes a debug sidecar container with UBI minimal. This sidecar provides debugging tools like `curl` without bloating the main application container. This is a common pattern for production debugging.

. Verify the pods are running:
+
[source,bash]
----
oc get pods -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                          READY   STATUS    RESTARTS   AGE
sample-app-xxxxx-xxxxx        2/2     Running   0          30s
sample-app-xxxxx-xxxxx        2/2     Running   0          30s
----
+
Note the `2/2` ready status indicates both the application and debug sidecar containers are running.

. Check what metrics the application exposes using the debug sidecar:
+
First, make a request to the application to initialize the metrics:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080
----
+
.Expected output
[source,text]
----
Hello from example application.
----
+
Now check the metrics endpoint:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080/metrics | head -20
----
+
The `-c debug` flag specifies which container to exec into (the debug sidecar). Since containers in the same pod share the network namespace, the sidecar can access `localhost:8080`.
+
.Sample output
[source,text]
----
# HELP http_request_duration_seconds Duration of all HTTP requests
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="found",method="get",le="0.005"} 1
http_request_duration_seconds_bucket{code="200",handler="found",method="get",le="0.01"} 1
...
# HELP http_requests_total Count of all HTTP requests
# TYPE http_requests_total counter
http_requests_total{code="200",method="get"} 1
# HELP version Version information about this binary
# TYPE version gauge
version{version="v0.3.0"} 1
----
+
The application exposes several metrics:
+
* `http_requests_total`: Counter of all HTTP requests by status code and method
* `http_request_duration_seconds`: Histogram of request latency
* `version`: Application version information
+
NOTE: Metrics are lazy-initialized and only appear after the first request. This is why we made an initial request before checking the metrics endpoint.

=== Verify

Check that your sample application is ready for monitoring:

* ✓ 2 pods running in {user}-observability-demo namespace
* ✓ Service exposing port 8080
* ✓ Metrics endpoint returns Prometheus-formatted data
* ✓ Metrics include `http_requests_total` counter

== Exercise 3: Configure ServiceMonitor to collect metrics

Now you'll configure Prometheus to scrape metrics from your sample application using a ServiceMonitor resource.

A **ServiceMonitor** tells Prometheus which services to scrape for metrics. It uses label selectors to find services and defines scrape intervals and ports.

=== Steps

. Create a ServiceMonitor resource:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sample-app-monitor
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
EOF
----
+
This ServiceMonitor tells Prometheus to:
+
* Find services with label `app: sample-app`
* Scrape the `http` port
* Collect metrics every 30 seconds
* Use the `/metrics` path

. Verify the ServiceMonitor was created:
+
[source,bash]
----
oc get servicemonitor -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                 AGE
sample-app-monitor   10s
----

. Generate some traffic to create metrics:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- sh -c 'for i in {1..50}; do curl -s localhost:8080 > /dev/null; done'
----
+
This sends 50 HTTP requests to the application using the debug sidecar, which will increment the `http_requests_total` metric.

. Wait 1-2 minutes for Prometheus to scrape the metrics, then query them:
+
Go to **Observe** → **Metrics** in the OpenShift console.
+
Enter this PromQL query:
+
[source,promql]
----
http_requests_total{namespace="{user}-observability-demo"}
----
+
Click **Run Queries**
+
You should see results showing the counter values for each pod.

. Query the rate of requests over time:
+
[source,promql]
----
rate(http_requests_total{namespace="{user}-observability-demo"}[5m])
----
+
This shows requests per second averaged over 5 minutes.

=== Verify

Check that Prometheus is collecting your application metrics:

* ✓ ServiceMonitor exists in {user}-observability-demo namespace
* ✓ PromQL query `http_requests_total` returns data
* ✓ Multiple time series (1 per pod) are visible
* ✓ Rate query shows request rate calculation

**What you learned**: ServiceMonitor resources configure Prometheus to scrape application metrics. Once configured, metrics are automatically collected and queryable via PromQL.

=== Troubleshooting

**Issue**: PromQL query returns no data

**Solution**: 
. Wait 1-2 minutes for Prometheus to scrape metrics
. Verify ServiceMonitor exists: `oc get servicemonitor -n {user}-observability-demo`
. Check that service labels match ServiceMonitor selector
. Verify pods are running and exposing metrics: `oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl localhost:8080/metrics`

**Issue**: Metrics show 0 requests

**Solution**: Generate traffic (run the curl loop again) and wait for Prometheus to scrape the updated values.

== Exercise 4: Create custom dashboards with COO

Querying metrics via PromQL is effective, but visualizing trends over time provides better insights. You'll use the Cluster Observability Operator (COO) to create custom dashboards.

=== Understanding COO vs user workload monitoring

Your cluster has **2 independent monitoring systems**:

**User Workload Monitoring (CMO)** (Exercise 1-3):

* Managed by Cluster Monitoring Operator (CMO)
* Cluster-wide monitoring for all namespaces
* Namespace: `openshift-user-workload-monitoring`
* Automatically discovers ServiceMonitors in any namespace
* Best for: Standard application monitoring with built-in OpenShift integration
* Limited customization options
* You've already used this for your sample-app

**Cluster Observability Operator (COO)** (Exercise 4):

* Independent operator that functions alongside CMO (no conflicts)
* Creates namespace-scoped monitoring stacks via MonitoringStack CR
* Namespace: `observability-demo` (can deploy to any namespace)
* Label-based ServiceMonitor discovery via `resourceSelector`
* Best for: Multi-tenant environments, custom dashboards, longer retention, team-scoped metrics
* Highly customizable (retention periods, storage, collection methods)
* Optional components: Thanos Querier, Alertmanager, UI plugins (Perses dashboards)

**Key difference**: COO uses a **label selector** (`resourceSelector.matchLabels`) to discover ServiceMonitors. Your sample-app ServiceMonitor from Exercise 3 doesn't have this label, so it's only visible to user workload monitoring (CMO), not COO.

**Why use both?**:

* CMO provides out-of-the-box monitoring for all applications
* COO provides advanced features: multi-tenancy, custom dashboards, longer retention
* They coexist independently - no conflicts

In this exercise, you'll create a **second** ServiceMonitor with the COO-specific label to demonstrate multi-tenant monitoring and custom Perses dashboards.

=== Steps

. Verify the COO MonitoringStack is deployed:
+
[source,bash]
----
oc get monitoringstack -n observability-demo
----
+
.Expected output
[source,text]
----
NAME                   AGE
observability-stack    1h
----

. Check the label selector the MonitoringStack uses to discover ServiceMonitors:
+
[source,bash]
----
oc get monitoringstack observability-stack -n observability-demo -o jsonpath='{.spec.resourceSelector.matchLabels}'
----
+
.Expected output
[source,json]
----
{"monitoring.rhobs/stack":"observability-stack"}
----
+
ServiceMonitors **must have this label** to be discovered by the COO stack.

. Create a **new** ServiceMonitor with the COO label, for the COO we need to use the monitoring.rhobs/v1 API group instead of the monitoring.coreos.com group used by CMO. This is because COO uses a different Prometheus operator under the hood that watches for ServiceMonitors with the `monitoring.rhobs/stack: observability-stack` label.
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.rhobs/v1
kind: ServiceMonitor
metadata:
  name: sample-app-coo
  namespace: {user}-observability-demo
  labels:
    app: sample-app
    monitoring.rhobs/stack: observability-stack
spec:
  selector:
    matchLabels:
      app: sample-app
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
EOF
----
+
Note the `monitoring.rhobs/stack: observability-stack` label - this tells COO to scrape this service.

. Verify the COO Prometheus stack has picked up your ServiceMonitor:
+
First, check that Prometheus pods are running:
+
[source,bash]
----
oc get pods -n observability-demo -l app.kubernetes.io/name=prometheus
----
+
.Expected output
[source,text]
----
NAME                                READY   STATUS    RESTARTS   AGE
prometheus-observability-stack-0    2/2     Running   0          1h
prometheus-observability-stack-1    2/2     Running   0          1h
prometheus-observability-stack-2    2/2     Running   0          1h
----
+
Wait 1-2 minutes for Prometheus to discover the new ServiceMonitor, then verify it's being scraped:
+
[source,bash]
----
oc exec -n observability-demo prometheus-observability-stack-0 -c prometheus -- \
  curl -s http://localhost:9090/api/v1/targets | \
  grep -E 'sample-app.*{user}-observability-demo' | head -5
----
+
.Expected output (showing your application is being monitored)
[source,text]
----
..."job":"sample-app-coo/sample-app/0","namespace":"{user}-observability-demo"...
----
+
If you see your application and namespace, the COO Prometheus has successfully discovered and is scraping your ServiceMonitor. If not, verify the ServiceMonitor has the correct label:
+
[source,bash]
----
oc get servicemonitor sample-app-coo -n {user}-observability-demo --show-labels
----
+
You should see `monitoring.rhobs/stack=observability-stack` in the labels.

. Create a Perses DataSource for the COO Prometheus:
+
A PersesDatasource creates a shared connection to the Prometheus instance that dashboards can reference.
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: perses.dev/v1alpha1
kind: PersesDatasource
metadata:
  name: {user}-prometheus
  namespace: {user}-observability-demo
  labels:
    monitoring.rhobs/stack: observability-stack
spec:
  config:
    default: true
    display:
      name: "{user} - Prometheus"
      description: "Direct connection to Prometheus in observability-demo MonitoringStack"
    plugin:
      kind: PrometheusDatasource
      spec:
        proxy:
          kind: HTTPProxy
          spec:
            url: 'http://observability-stack-prometheus.observability-demo.svc.cluster.local:9090'
EOF
----
+
This creates a datasource pointing to the COO Prometheus service. Dashboards will reference this datasource instead of embedding connection details.

. Create a Perses dashboard to visualize your metrics:
+
Now create a dashboard that references the datasource:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: perses.dev/v1alpha1
kind: PersesDashboard
metadata:
  name: sample-app-dashboard
  namespace: {user}-observability-demo
  labels:
    monitoring.rhobs/stack: observability-stack
spec:
  display:
    name: Sample Application Metrics
    description: HTTP request metrics for sample-app
  duration: 1h
  panels:
    httpRequestRate:
      kind: Panel
      spec:
        display:
          name: HTTP Request Rate
          description: Requests per second by pod
        plugin:
          kind: TimeSeriesChart
          spec:
            legend:
              position: bottom
        queries:
          - kind: TimeSeriesQuery
            spec:
              plugin:
                kind: PrometheusTimeSeriesQuery
                spec:
                  datasource:
                    kind: PrometheusDatasource
                    name: {user}-prometheus
                  query: sum(rate(http_requests_total{namespace="{user}-observability-demo"}[5m])) by (pod)
    totalRequests:
      kind: Panel
      spec:
        display:
          name: Total HTTP Requests
          description: Cumulative request count by pod
        plugin:
          kind: TimeSeriesChart
          spec:
            legend:
              position: bottom
        queries:
          - kind: TimeSeriesQuery
            spec:
              plugin:
                kind: PrometheusTimeSeriesQuery
                spec:
                  datasource:
                    kind: PrometheusDatasource
                    name: {user}-prometheus
                  query: sum(http_requests_total{namespace="{user}-observability-demo"}) by (pod)
  layouts:
    - kind: Grid
      spec:
        display:
          title: Request Metrics
          collapse:
            open: true
        items:
          - x: 0
            y: 0
            width: 12
            height: 6
            content:
              \$ref: '#/spec/panels/httpRequestRate'
          - x: 0
            y: 6
            width: 12
            height: 6
            content:
              \$ref: '#/spec/panels/totalRequests'
EOF
----
+
This creates a Perses dashboard with 2 panels that reference the shared datasource: request rate and total requests.

. Generate traffic to populate the dashboard:
+
[source,bash]
----
oc exec -n {user}-observability-demo deployment/sample-app -c debug -- sh -c 'for i in {1..100}; do curl -s localhost:8080 > /dev/null; sleep 0.1; done'
----

. Access your custom dashboard in the OpenShift console:
+
* Navigate to **Observe** → **Dashboards**
* In the dashboard dropdown, select **Sample Application Metrics**
* You should see 2 panels showing request rate and total requests

. Verify the dashboard is using COO metrics:
+
The dashboard queries the COO Prometheus service in the `observability-demo` namespace through the Perses datasource (not user workload monitoring).

=== Verify

Check that your COO dashboard is working:

* ✓ MonitoringStack exists in observability-demo namespace
* ✓ ServiceMonitor `sample-app-coo` has label `monitoring.rhobs/stack: observability-stack`
* ✓ PersesDashboard created successfully
* ✓ Dashboard appears in **Observe** → **Dashboards**
* ✓ Panels show request rate and total requests
* ✓ Graphs update with new data

**What you learned**: 

* COO is an independent operator that coexists with the Cluster Monitoring Operator (CMO)
* MonitoringStack CR creates namespace-scoped monitoring with label-based ServiceMonitor discovery
* COO provides multi-tenancy: different teams can have isolated monitoring stacks
* Perses dashboards are declarative (YAML) and versioned with your application
* You can run multiple monitoring systems: cluster-wide (CMO) + team-scoped (COO)
* COO offers more flexibility: longer retention, custom configurations, independent release cycles
* Thanos Querier aggregates metrics from multiple Prometheus replicas for high availability

=== Troubleshooting

**Issue**: ServiceMonitor not picked up by COO

**Solution**:
. Verify the label: `oc get servicemonitor sample-app-coo -n {user}-observability-demo --show-labels`
. Must have: `monitoring.rhobs/stack=observability-stack`
. Wait 1-2 minutes for Prometheus to discover the new ServiceMonitor after creation

**Issue**: Dashboard not visible in console

**Solution**:
. Verify PersesDashboard exists: `oc get persesdashboard -n {user}-observability-demo`
. Check for errors: `oc describe persesdashboard sample-app-dashboard -n {user}-observability-demo`
. Wait 1-2 minutes for the dashboard to appear in the console after creation

**Issue**: Dashboard shows "No data"

**Solution**:
. Wait 2-3 minutes for Prometheus to scrape metrics
. Verify Thanos Querier is running: `oc get pods -n observability-demo -l app.kubernetes.io/name=thanos-querier`
. Generate traffic (run the curl loop again)
. Check Prometheus targets: Port-forward to COO Prometheus and check `/targets`

== Exercise 5: Configure alerting rules

Dashboards help you see current state, but alerts proactively notify you when problems occur. You'll create an alerting rule that fires when request rates drop unexpectedly.

=== Steps

. Create a PrometheusRule resource with an alerting rule:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sample-app-alerts
  namespace: {user}-observability-demo
  labels:
    app: sample-app
spec:
  groups:
  - name: sample-app
    interval: 30s
    rules:
    - alert: LowRequestRate
      expr: sum(rate(http_requests_total{namespace="{user}-observability-demo"}[5m])) < 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Low request rate detected"
        description: "Application in namespace {{ \$labels.namespace }} is receiving fewer than 0.1 requests/second for more than 2 minutes."
EOF
----
+
This alert fires if request rate drops below 0.1 requests/second for more than 2 minutes.

. Verify the PrometheusRule was created:
+
[source,bash]
----
oc get prometheusrule -n {user}-observability-demo
----

. Check alert status in the console:
+
Navigate to **Observe** → **Alerting** in the OpenShift console.
+
* Click on the **Alerting rules** tab
* Search for "LowRequestRate"
* You should see the alert in **Inactive** or **Pending** state

. Trigger the alert by stopping traffic:
+
Wait 2-3 minutes without generating any traffic. The alert should transition from **Inactive** → **Pending** → **Firing**.

. Generate traffic to resolve the alert:
+
[source,bash]
----
for i in {1..200}; do
  oc exec -n {user}-observability-demo deployment/sample-app -c debug -- curl -s localhost:8080 > /dev/null
  sleep 0.5
done
----
+
After a few minutes, the alert should transition back to **Inactive**.

=== Verify

Check that your alerting rules are working:

* ✓ PrometheusRule exists in {user}-observability-demo namespace
* ✓ Alert appears in **Observe** → **Alerting** → **Alerting rules**
* ✓ Alert transitions through states: Inactive → Pending → Firing
* ✓ Alert resolves when traffic increases

**What you learned**: PrometheusRule resources define alerting conditions. Alerts transition through states (Inactive → Pending → Firing) based on PromQL expressions and duration thresholds.

=== Troubleshooting

**Issue**: Alert not visible in console

**Solution**:
. Verify PrometheusRule exists: `oc get prometheusrule -n {user}-observability-demo`
. Check syntax errors in PromQL expression
. Wait 1-2 minutes for Prometheus to reload configuration

**Issue**: Alerting page shows **Restricted access** with `prometheuses/api` or `alertmanagers/api` forbidden

**Solution**:
. This indicates missing RBAC for monitoring API subresources used by the OpenShift console
. Ask a cluster admin to run `make deploy` (with cluster-admin credentials) so workshop monitoring API RoleBindings are applied
. Verify access after RBAC update:
+
[source,bash]
----
oc auth can-i get prometheuses.monitoring.coreos.com/api -n openshift-user-workload-monitoring
oc auth can-i get alertmanagers.monitoring.coreos.com/api -n openshift-user-workload-monitoring
----
. Refresh the Alerting page after both checks return `yes`

**Issue**: Alert never fires

**Solution**:
. Verify condition is met (request rate < 0.1/sec)
. Check `for` duration (must be in low state for 2 minutes)
. Generate traffic then stop to test alert firing

== Learning outcomes

By completing this module, you should now understand:

* ✓ The 3 pillars of observability (metrics, logs, traces) and when to use each signal type
* ✓ Observability methodologies: RED (request-driven), USE (resources), Golden Signals
* ✓ OpenShift's monitoring architecture: Platform (CMO), User Workload (CMO), and COO
* ✓ Cluster Observability Operator (COO): Independent multi-tenant monitoring with MonitoringStack CR
* ✓ How ServiceMonitor resources configure Prometheus to scrape application metrics
* ✓ Writing PromQL queries to analyze application performance and calculate rates
* ✓ Creating declarative Perses dashboards for custom visualizations with COO
* ✓ Configuring PrometheusRule resources to proactively alert on application issues

**Business impact**:

You've implemented the foundation of proactive monitoring. Instead of discovering issues through user complaints, you now have:

* Real-time visibility into application request rates and patterns
* Declarative dashboards versioned alongside your application code
* Team-scoped monitoring (COO) isolated from cluster infrastructure monitoring
* Automated alerts that notify you before problems impact customers

**Next steps**: Module 2 will add centralized logging with LokiStack, enabling you to correlate metrics with detailed log messages for faster root cause analysis.

== Module summary

You successfully demonstrated how user workload monitoring provides application visibility.

**What you accomplished**:

* Verified the user workload monitoring stack (Prometheus, Thanos, Alertmanager)
* Deployed a sample application with Prometheus-compatible metrics
* Configured ServiceMonitor to automatically collect application metrics
* Created namespace-scoped monitoring with Cluster Observability Operator (COO)
* Built declarative Perses dashboards to visualize application performance
* Implemented alerting rules to proactively detect application issues

**Key concepts mastered**:

* **ServiceMonitor**: Declaratively configures which services Prometheus scrapes
* **MonitoringStack**: Namespace-scoped monitoring with label-based resource discovery
* **PromQL**: Query language for analyzing time-series metrics data
* **PersesDashboard**: Declarative YAML-based dashboard as code
* **PrometheusRule**: Defines alerting conditions and notification thresholds
* **User workload monitoring vs COO**: Cluster-wide vs namespace-scoped monitoring

**Metrics collected**:

* `http_requests_total`: Counter of all HTTP requests by status code
* `up`: Target availability (1 = healthy, 0 = down)
* Custom application metrics from your deployed services

Continue to Module 2 to add centralized logging capabilities.
