= Module 2: Logging with LokiStack
:source-highlighter: rouge
:toc: macro
:toclevels: 1

After implementing user workload monitoring in Module 1, you now need to address log management challenges. While metrics show when issues occur, investigating root causes requires efficient log analysis across distributed services.

Your organization runs a distributed application with 12+ microservices. When issues occur, developers currently access individual pod logs manually. This process is time-consuming and error-prone, especially when correlating events across multiple services.

In this module, you'll implement centralized logging with LokiStack and learn to write LogQL queries that quickly surface relevant log entries across your entire application.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand LokiStack architecture and how it differs from traditional logging solutions
* Configure ClusterLogForwarder to send application logs to Loki
* Write LogQL queries to search and filter logs across multiple services
* Correlate logs with metrics to accelerate troubleshooting
* Create log-based alerts for critical application events

== Understanding LokiStack architecture

Before configuring logging, you need to understand how LokiStack works and why it's efficient for Kubernetes environments.

=== What is Loki?

**Loki** is a horizontally scalable, highly available log aggregation system inspired by Prometheus. Unlike traditional logging systems that index the full text of log messages, Loki only indexes metadata (labels) and stores log content separately.

**Key differences from traditional logging**:

* **Traditional (Elasticsearch)**: Indexes every word in every log message → high storage cost, slower writes
* **Loki**: Indexes only labels (namespace, pod, container) → lower storage cost, faster writes, efficient for Kubernetes

**LokiStack components**:

* **Distributor**: Receives log streams from clients
* **Ingester**: Writes logs to storage and serves recent queries
* **Querier**: Handles LogQL queries against stored logs
* **Compactor**: Maintains index and deletes expired logs

=== Log forwarding flow

In OpenShift, logs flow through this pipeline:

. Application writes logs to stdout/stderr
. Container runtime captures logs to node filesystem
. OpenShift Logging Operator's collector pods read logs
. ClusterLogForwarder routes logs to LokiStack
. Loki stores logs with labels for querying

You'll configure the ClusterLogForwarder to ensure application logs reach Loki.

== Exercise 1: Verify logging infrastructure

You need to verify that the logging stack was deployed via GitOps and is ready to receive logs.

The LokiStack instance and logging operator were pre-configured, so you'll confirm the components are running.

=== Steps

. Log into the OpenShift console at {openshift_console_url} (if not already logged in)

. Verify the logging operator is running:
+
[source,bash]
----
oc get pods -n openshift-logging
----
+
.Expected output
[source,text]
----
NAME                                           READY   STATUS    RESTARTS   AGE
cluster-logging-operator-xxxxx                 1/1     Running   0          1h
collector-xxxxx                                2/2     Running   0          1h
collector-xxxxx                                2/2     Running   0          1h
logging-loki-compactor-0                       1/1     Running   0          1h
logging-loki-distributor-xxxxx                 1/1     Running   0          1h
logging-loki-gateway-xxxxx                     2/2     Running   0          1h
logging-loki-index-gateway-0                   1/1     Running   0          1h
logging-loki-ingester-0                        1/1     Running   0          1h
logging-loki-querier-xxxxx                     1/1     Running   0          1h
logging-loki-query-frontend-xxxxx              1/1     Running   0          1h
----

. Check the LokiStack instance:
+
[source,bash]
----
oc get lokistack -n openshift-logging
----
+
.Expected output
[source,text]
----
NAME           AGE   SIZE             MODE      STATUS
logging-loki   1h    1x.extra-small   static    Ready
----
+
The status should be **Ready**.

. Verify the ClusterLogForwarder configuration:
+
[source,bash]
----
oc get clusterlogforwarder -n openshift-logging
----
+
.Expected output
[source,text]
----
NAME       AGE
instance   1h
----

. View the ClusterLogForwarder configuration:
+
[source,bash]
----
oc get clusterlogforwarder instance -n openshift-logging -o yaml
----
+
Look for pipelines that forward application and infrastructure logs to the default output (LokiStack).

=== Verify

Check that your logging infrastructure is operational:

* ✓ Logging operator pod is Running
* ✓ Collector pods are Running on each node
* ✓ LokiStack components (distributor, ingester, querier) are Running
* ✓ LokiStack status is Ready
* ✓ ClusterLogForwarder exists with pipelines configured

**What you learned**: OpenShift Logging uses a collector (based on Vector) to gather logs from all pods and forward them to LokiStack according to ClusterLogForwarder rules.

=== Troubleshooting

**Issue**: LokiStack status shows "Degraded" or "Pending"

**Solution**:
. Check LokiStack pods for errors: `oc get pods -n openshift-logging | grep loki`
. Describe the LokiStack: `oc describe lokistack logging-loki -n openshift-logging`
. If reason is `MissingObjectStorageSecret`, check sync Job status:
+
[source,bash]
----
oc get job logging-loki-s3-secret-sync -n openshift-logging
oc logs job/logging-loki-s3-secret-sync -n openshift-logging
----
. Check for storage issues: `oc get pvc -n openshift-logging`
. Verify storage class exists: `oc get storageclass`

**Issue**: No collector pods running

**Solution**:
. Check DaemonSet status: `oc get daemonset -n openshift-logging`
. View collector logs: `oc logs -n openshift-logging -l app.kubernetes.io/component=collector --tail=100`
. Verify ClusterLogForwarder resource exists: `oc get clusterlogforwarder -n openshift-logging`

**Issue**: ClusterLogForwarder not found

**Solution**: The logging stack may not be fully configured. Check that the GitOps deployment completed successfully.

== Exercise 2: Generate application logs

To practice querying logs, you need an application that generates diverse log entries. In this exercise, you'll deploy a simple three-service Go application:

* `frontend` receives HTTP requests and forwards them to `backend`
* `backend` writes request events to `database`
* `database` exposes a REST API backed by persistent ChaiSQL storage on disk

Each service writes structured request logs to stdout, which Loki collects.

=== Steps

. Verify Shipwright build APIs are available:
+
[source,bash]
----
oc api-resources | grep -E 'builds.shipwright.io|buildruns.shipwright.io'
oc get clusterbuildstrategy buildah
----
+
If either command fails, stop here and ask your workshop administrator to enable the OpenShift Builds (Shipwright) operator.

. Deploy the logging demo application manifest from GitHub:
+
[source,bash]
----
NAMESPACE="{user}-observability-demo"
curl -sL https://raw.githubusercontent.com/cldmnky/observability-workshop/main/src/deploy.yaml \
  | sed "s/__NAMESPACE__/${NAMESPACE}/g" \
  | oc apply -f -
----

. Wait for image builds to complete:
+
[source,bash]
----
oc wait --for=condition=Succeeded -n {user}-observability-demo buildrun/frontend-buildrun --timeout=20m
oc wait --for=condition=Succeeded -n {user}-observability-demo buildrun/backend-buildrun --timeout=20m
oc wait --for=condition=Succeeded -n {user}-observability-demo buildrun/database-buildrun --timeout=20m
----

. Verify the three deployments are running:
+
[source,bash]
----
oc get pods -n {user}-observability-demo -l app=frontend
oc get pods -n {user}-observability-demo -l app=backend
oc get pods -n {user}-observability-demo -l app=database
----

. Verify the frontend Route and open the UI:
+
[source,bash]
----
oc get route frontend -n {user}-observability-demo
FRONTEND_HOST=$(oc get route frontend -n {user}-observability-demo -o jsonpath='{.spec.host}')
echo "https://${FRONTEND_HOST}"
----

. Open the frontend URL in your browser, create at least two workshop notes, and click **Export Markdown**.
+
The exported file (`workshop-notes.md`) confirms notes are persisted and downloadable from the frontend.

. Generate traffic with mixed success and failure responses:
+
[source,bash]
----
oc run -n {user}-observability-demo traffic-generator --rm -i --restart=Never --image=quay.io/curl/curl:8.11.1 -- \
  sh -c 'for i in $(seq 1 50); do curl -s http://frontend:8080/ping >/dev/null; curl -s -o /dev/null http://frontend:8080/error; echo "Completed request pair $i"; sleep 0.5; done'
----
+
This generates a mix of 200 (success) and 404 (error) responses, creating varied log entries.

. View logs from each tier to see request traces:
+
[source,bash]
----
oc logs -n {user}-observability-demo deployment/frontend --tail=20
oc logs -n {user}-observability-demo deployment/backend --tail=20
oc logs -n {user}-observability-demo deployment/database --tail=20
----
+
.Sample output
[source,text]
----
2026/02/11 10:23:14 service=frontend method=POST path=/api/notes status=201 duration_ms=5
2026/02/11 10:23:15 service=frontend method=GET path=/ping status=200 duration_ms=3
2026/02/11 10:23:16 service=frontend method=GET path=/error status=404 duration_ms=4
2026/02/11 10:23:14 service=backend method=POST path=/api/notes status=201 duration_ms=3
2026/02/11 10:23:16 service=backend method=GET path=/api/error status=404 duration_ms=2
2026/02/11 10:23:14 service=database method=POST path=/notes status=201 duration_ms=1
2026/02/11 10:23:16 service=database method=POST path=/events status=201 duration_ms=1
----

=== Verify

Check that logs are being generated:

* ✓ `frontend` logs include `/ping` (200) and `/error` (404)
* ✓ `backend` logs show forwarded `/api/ok` and `/api/error` requests
* ✓ Frontend Route is reachable and serves the notes UI
* ✓ Notes can be exported from the frontend as `workshop-notes.md`
* ✓ `database` logs show persisted `/events` API calls
* ✓ Logs visible via `oc logs` command
* ✓ Multiple applications are running in the {user}-observability-demo namespace

**What you learned**: Applications write logs to stdout/stderr, which OpenShift captures and forwards to your logging backend. Multiple services can run in the same namespace.

== Exercise 3: Query logs with LogQL

Now you'll access the Loki web interface and write LogQL queries to search logs across all your applications.

**LogQL** is Loki's query language, similar to PromQL but for logs instead of metrics.

=== Steps

. Access the OpenShift console and navigate to **Observe** → **Logs**
+
This opens the log query interface powered by Loki.

. Write your first LogQL query to see all logs from your namespace:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"}
----
+
Click **Run query**
+
You should see log entries from all pods in the {user}-observability-demo namespace.

. Filter logs to show only error responses (404):
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} |= "404"
----
+
The `|=` operator filters for log lines containing "404".

. Query logs from a specific pod:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo", kubernetes_pod_name=~"frontend.*"}
----
+
The `=~` operator matches a regular expression.

. Query frontend note activity generated from the UI:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo", kubernetes_pod_name=~"frontend.*"} |= "/api/notes"
----

This shows note creation and note update calls from the frontend route.

. Combine multiple filters to find successful frontend requests:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo", kubernetes_pod_name=~"frontend.*"} |= "path=/ping" |= "status=200"
----
+
This shows successful `/ping` requests from the frontend pod.

. Use LogQL to count error rates:
+
[source,logql]
----
sum(rate({kubernetes_namespace_name="{user}-observability-demo"} |= "404" [5m]))
----
+
This calculates the rate of 404 errors per second over the last 5 minutes.

. Query logs from multiple applications:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} | line_format "{{.kubernetes_pod_name}}: {{ __line__ }}"
----
+
This formats output to show which pod generated each log line and works for plain-text logs.

=== Verify

Check that you can query logs effectively:

* ✓ Logs from {user}-observability-demo namespace are visible
* ✓ Filter queries return only matching log lines
* ✓ Pod-specific queries work with regex patterns
* ✓ Rate calculations show error frequency
* ✓ Logs from multiple applications can be queried together

**What you learned**: LogQL uses labels (like namespace, pod name) to select log streams, then filters content with operators like `|=` (contains) and `!=` (does not contain).

=== Troubleshooting

**Issue**: No logs appear in query results

**Solution**:
. Wait 1-2 minutes for logs to be ingested by Loki
. Verify pods are running: `oc get pods -n {user}-observability-demo`
. Check collector is forwarding logs: `oc logs -n openshift-logging -l app.kubernetes.io/component=collector --tail=100`
. Verify time range in query interface (expand to last 1 hour)

**Issue**: Query returns "Error: parse error"

**Solution**:
. Check LogQL syntax (labels in braces, filters with pipes)
. Ensure label names are correct: `kubernetes_namespace_name` not `namespace`
. Use quotes around label values: `{kubernetes_namespace_name="{user}-observability-demo"}`

**Issue**: Rate queries return no data

**Solution**:
. Ensure enough matching logs exist (generate more traffic)
. Check time range: `[5m]` requires data in last 5 minutes
. Verify filter matches log content: `|= "404"` requires "404" in log lines

== Exercise 4: Correlate logs with metrics

The real power of observability comes from correlating different signals. You'll use metrics from Module 1 alongside logs to diagnose an issue faster.

=== Steps

. Generate a burst of errors to simulate an incident:
+
[source,bash]
----
oc run -n {user}-observability-demo error-generator --rm -i --restart=Never --image=quay.io/curl/curl:8.11.1 -- \
  sh -c 'for i in $(seq 1 100); do curl -s -o /dev/null http://frontend:8080/error; sleep 0.1; done'
----

. Check metrics to detect the traffic spike:
+
Navigate to **Observe** → **Metrics**
+
Query:
+
[source,promql]
----
sum(rate(container_network_receive_bytes_total{namespace="{user}-observability-demo",pod=~"frontend-.*"}[5m]))
----
+
You should see a traffic spike while the error generator is running.

. Switch to **Observe** → **Logs** and correlate with log data:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} |= "404" |= "error"
----
+
This shows the actual error log messages during the spike.

. Use time range alignment to see the correlation:
+
In the logs interface, set the time range to match when you saw the metric spike (last 15 minutes).
+
Compare the timestamp of log entries with the metric spike time.

. Extract structured data from logs:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} 
  |= "404" 
  | regexp `(?P<timestamp>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}).*(?P<method>\w+).*(?P<status>\d{3})`
  | line_format "{{.timestamp}} - {{.method}} {{.status}}"
----
+
This extracts timestamp, HTTP method, and status code from log lines.

=== Verify

Check that you can correlate metrics and logs:

* ✓ Metric query shows frontend traffic spike
* ✓ Log query reveals error messages during the same time period
* ✓ Time ranges align between metrics and logs views
* ✓ Structured data can be extracted from log lines
* ✓ Correlation reduces investigation time from minutes to seconds

**What you learned**: Metrics tell you when and where problems occur. Logs provide the detailed context explaining why. Together, they reduce mean time to resolution through unified analysis.

=== Troubleshooting

**Issue**: Metric and log timestamps don't align

**Solution**:
. Check timezone settings in console (both views should use same timezone)
. Verify system clocks are synchronized across cluster nodes
. Allow 30-60 seconds for metric scraping and log ingestion delays

**Issue**: Cannot extract structured data with regexp

**Solution**:
. Verify log format matches regex pattern
. View raw logs first: `{kubernetes_namespace_name="{user}-observability-demo"}`
. Test regex pattern against actual log samples
. Use simpler filters first, then add complexity

== Exercise 5: Create log-based alerts

Just as you created metric-based alerts in Module 1, you can create alerts based on log patterns. You'll configure an alert that fires when error rates exceed thresholds.

=== Steps

. Create an AlertingRule that triggers on high error log rates:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: loki.grafana.com/v1
kind: AlertingRule
metadata:
  name: high-error-rate
  namespace: openshift-logging
  labels:
    app: loki
spec:
  tenantID: application
  groups:
  - name: application-errors
    interval: 30s
    rules:
    - alert: HighApplicationErrorRate
      expr: |
        sum(rate({kubernetes_namespace_name="{user}-observability-demo"} |= "404" [5m])) > 0.5
      for: 2m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High error rate in {user}-observability-demo namespace"
        description: "Application in {user}-observability-demo is producing more than 0.5 errors/second for 2 minutes."
EOF
----
+
This alert fires if error logs exceed 0.5 entries per second for 2 minutes.

. Verify the AlertingRule was created:
+
[source,bash]
----
oc get alertingrule -n openshift-logging
----

. Trigger the alert by generating sustained errors:
+
[source,bash]
----
oc run -n {user}-observability-demo alert-generator --rm -i --restart=Never --image=quay.io/curl/curl:8.11.1 -- \
  sh -c 'for i in $(seq 1 200); do curl -s -o /dev/null http://frontend:8080/error; sleep 0.5; done'
----
+
This generates errors for about 100 seconds, which should trigger the alert after 2 minutes.

. Check alert status in the console:
+
Navigate to **Observe** → **Alerting** → **Alerting rules**
+
Search for "HighApplicationErrorRate"
+
The alert should transition: **Inactive** → **Pending** (after 2 min) → **Firing** (if errors continue)

. Stop generating errors and watch the alert resolve:
+
After stopping the error generation, wait 2-3 minutes and the alert should return to **Inactive**.

=== Verify

Check that log-based alerting works:

* ✓ AlertingRule resource created successfully
* ✓ Alert visible in OpenShift console alerting interface
* ✓ Alert transitions to Pending when errors increase
* ✓ Alert fires when error rate exceeds threshold for duration
* ✓ Alert resolves when error rate drops below threshold

**What you learned**: Loki supports alerting based on log patterns and rates. This allows you to proactively detect issues based on log content, not just metrics.

=== Troubleshooting

**Issue**: AlertingRule not found or not created

**Solution**:
. Check API availability: `oc api-resources | grep alertingrule`
. If API not available, check Loki Operator version
. Verify namespace: AlertingRules must be in openshift-logging for Loki
. Check for syntax errors: `oc describe alertingrule high-error-rate -n openshift-logging`

**Issue**: Alert never fires

**Solution**:
. Verify LogQL expression matches actual logs: Test in **Observe** → **Logs** first
. Ensure error rate actually exceeds threshold (0.5/sec)
. Check `for` duration: Must exceed threshold for full 2 minutes
. View Loki ruler logs: `oc logs -n openshift-logging -l app.kubernetes.io/component=ruler`

**Issue**: Alert shows in Loki but not in OpenShift Alerting UI

**Solution**:
. Log-based alerts may only appear in Loki-specific interfaces
. Check Loki rules: `oc get alertingrule -n openshift-logging`
. For integration with Alertmanager, additional configuration may be needed

== Learning outcomes

By completing this module, you should now understand:

* ✓ LokiStack's label-based indexing approach reduces storage costs compared to full-text indexing
* ✓ ClusterLogForwarder routes application logs from pods to Loki for centralized storage
* ✓ LogQL queries use labels to select streams and filters to search log content
* ✓ Correlating logs with metrics reduces troubleshooting time through unified analysis
* ✓ Log-based alerts can detect issues based on patterns in log messages

**Business impact**:

You've eliminated the manual log searching process. Instead of accessing individual pods and searching files manually, you now have:

* Centralized logs from all 12+ microservices in 1 queryable system
* Ability to search across all services simultaneously with LogQL
* Correlation between metric spikes and log messages
* Automated alerts on critical log patterns

**Estimated time savings**: Reduced log investigation time from 30-60 minutes to 2-5 minutes per incident.

**Next steps**: Module 3 will add distributed tracing, enabling you to follow individual requests across all microservices to identify performance bottlenecks.

== Module summary

You successfully implemented centralized logging with LokiStack.

**What you accomplished**:

* Verified the LokiStack logging infrastructure deployed via GitOps
* Generated diverse application logs across multiple services
* Wrote LogQL queries to filter and search logs across distributed applications
* Correlated metric spikes with detailed log messages for faster root cause analysis
* Created log-based alerts to proactively detect error patterns

**Key concepts mastered**:

* **LokiStack**: Label-based log aggregation system optimized for Kubernetes
* **ClusterLogForwarder**: Routes logs from pods to configured outputs (Loki)
* **LogQL**: Query language for searching and filtering log streams
* **Log correlation**: Combining logs with metrics for comprehensive troubleshooting
* **Log-based alerting**: Detecting issues based on log content patterns

**LogQL operators learned**:

* `{label="value"}`: Select log streams by label
* `|=`: Filter for lines containing text
* `!=`: Exclude lines containing text
* `|~`: Regex match filter
* `rate()`: Calculate rate of log entries
* `regexp`: Extract structured data from logs

Continue to Module 3 to add distributed tracing capabilities.
