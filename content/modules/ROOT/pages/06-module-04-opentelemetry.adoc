= Module 4: OpenTelemetry integration
:source-highlighter: rouge
:toc: macro
:toclevels: 1

In Module 3 you verified the Tempo tracing backend, enabled the Distributed Tracing UI plugin, and explored how the Go application is instrumented with the OpenTelemetry SDK. The telemetry pipeline was not yet active—the application was running silently.

In this module you will activate that pipeline end-to-end. You'll deploy a sidecar OpenTelemetry Collector and an `Instrumentation` CR in your namespace, enable the SDK on all three services, and observe live traces, metrics, and logs flowing from your application through the collector pipeline to Tempo, Prometheus, and Loki.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand the sidecar-to-central OpenTelemetry Collector architecture
* Create a sidecar `OpenTelemetryCollector` CR and configure its pipeline
* Create an `Instrumentation` CR for zero-code agent injection
* Enable OpenTelemetry on the workshop application using `enable-otel.yaml`
* Observe live traces in the **Observe → Traces** console UI
* Understand how the central collector fans out telemetry to Tempo, Prometheus, and Loki
* Enable zero-code Python auto-instrumentation on the **notifier** service using the `Instrumentation` CR
* Observe a four-hop trace spanning Go and Python services in a single waterfall

== Understanding the collector architecture

Before enabling telemetry, understand how data flows from the application to each observability backend.

=== Two-tier collector pattern

This workshop uses a two-tier OpenTelemetry Collector topology. Every signal—traces, metrics, and logs—flows through the same two hops before reaching a purpose-built backend:

[source,text]
----
Application pods ({user}-observability-demo)
  +--------------------------------------------------------+
  |  frontend / backend / database / notifier              |
  |  +----------+  OTLP HTTP (localhost:4318)              |
  |  | app      |---> otc-container (sidecar collector)   |
  |  +----------+                                          |
  +--------------------------------------------------------+
         | All signals: traces + metrics + logs
         | OTLP gRPC (cluster DNS, port 4317)
         v
  central-collector-collector.observability-demo.svc:4317
  +------------------------------------------------------------------+
  |  Central collector – deployment mode, 2 replicas                 |
  |  (namespace: observability-demo)                                 |
  |                                                                  |
  |  Signal routing:                                                 |
  |  traces  --> otlp/tempo     --> TempoStack distributor :4317     |
  |              (TLS + bearer token + X-Scope-OrgID: dev)           |
  |  traces  --> spanmetrics    --> traces/spanmetrics pipeline       |
  |              connector (RED metrics from spans)                  |
  |  metrics --> prometheusremotewrite                               |
  |              --> COO Prometheus /api/v1/write :9090              |
  |  span    --> transform (rename) + prometheusremotewrite          |
  |  metrics     --> COO Prometheus /api/v1/write :9090              |
  |  logs    --> resource/logs + transform/logs + otlphttp/logs      |
  |              --> LokiStack gateway :8080 (application tenant)    |
  +------------------------------------------------------------------+
----

**Why two tiers?**

* **Sidecar collector**: Runs within the application pod as a second container (`otc-container`). The app sends to `localhost:4318` (no network hop). The sidecar enriches every span, metric data point, and log record with Kubernetes metadata (`k8s.pod.name`, `k8s.deployment.name`, `k8s.namespace.name`) via the `k8sattributes` processor, then forwards all three signals over a single gRPC connection to the central collector.
* **Central collector**: Runs as a shared `Deployment` (2 replicas) in `observability-demo`. It receives all signals from every user's sidecar and routes them to different backends using different protocols and auth mechanisms. It also runs the `spanmetrics` connector, which generates RED metrics directly from incoming trace spans.

This pattern keeps the sidecar simple (no secrets, no TLS config, no auth tokens) while centralising the complex backend integrations in one place.

=== Processors in the sidecar collector

The sidecar uses four processors in order:

[options="header",cols="25,75"]
|===
|Processor |Function
|`memory_limiter` |Prevents the collector from consuming more than 75% of available pod memory
|`resourcedetection` |Detects OpenShift infrastructure attributes (such as `k8s.cluster.name`) and adds them to all telemetry
|`k8sattributes` |Calls the Kubernetes API to attach pod name, namespace, deployment name, node name, and pod UID to every span, metric, and log record
|`batch` |Accumulates records before sending to reduce network overhead
|===

=== Span metrics connector in the central collector

The central collector uses a `spanmetrics` connector to generate **RED metrics** automatically from incoming traces:

* **Rate**: `traces_spanmetrics_calls_total` — request count per service and operation
* **Errors**: `traces_spanmetrics_calls_total{status.code="STATUS_CODE_ERROR"}` — error count
* **Duration**: `traces_spanmetrics_latency_bucket` — latency histogram with configurable buckets

These metrics are published to the COO-managed Prometheus instance via `prometheusremotewrite`. The COO `MonitoringStack` has `enableRemoteWriteReceiver: true` set, which activates the `/api/v1/write` endpoint that Prometheus exposes for ingest. Without that flag the push would be silently rejected with HTTP 404.

== Exercise 1: Verify the OpenTelemetry operator and pre-deployed components

Before creating resources in your namespace, confirm the OpenTelemetry Operator and the shared `observability-demo` infrastructure are healthy.

=== Steps

. Verify the OpenTelemetry Operator pod is running:
+
[source,bash]
----
oc get pods -n openshift-operators \
  -l app.kubernetes.io/name=opentelemetry-operator
----
+
.Expected output
[source,text]
----
NAME                                          READY   STATUS    RESTARTS   AGE
opentelemetry-operator-controller-xxxxx       2/2     Running   0          1h
----

. Confirm the operator registered the CRDs:
+
[source,bash]
----
oc api-resources | grep opentelemetry
----
+
.Expected output
[source,text]
----
instrumentations              opentelemetry.io   true    Instrumentation
opentelemetrycollectors       opentelemetry.io   true    OpenTelemetryCollector
----

. Inspect the pre-deployed central collector:
+
[source,bash]
----
oc get opentelemetrycollector central-collector -n observability-demo
----
+
.Expected output
[source,text]
----
NAME               MODE         VERSION
central-collector  deployment   0.140.0-2
----

. Verify the central collector pods are running:
+
[source,bash]
----
oc get pods -n observability-demo \
  -l app.kubernetes.io/name=central-collector-collector
----
+
.Expected output
[source,text]
----
NAME                                      READY   STATUS    RESTARTS   AGE
central-collector-collector-xxxxx         1/1     Running   0          1h
central-collector-collector-xxxxx         1/1     Running   0          1h
----
+
Two replicas provide resilience for the shared collection endpoint.

. Inspect the central collector service (this is where sidecars forward telemetry):
+
[source,bash]
----
oc get svc central-collector-collector -n observability-demo
----
+
.Expected output
[source,text]
----
NAME                           TYPE        CLUSTER-IP      PORT(S)
central-collector-collector    ClusterIP   172.30.x.x      4317/TCP, 4318/TCP
----

=== Verify

* ✓ OpenTelemetry Operator pod is Running (2/2)
* ✓ `OpenTelemetryCollector` and `Instrumentation` CRDs are registered
* ✓ `central-collector` exists in `observability-demo` in deployment mode with 2 replicas
* ✓ `central-collector-collector` service exposes ports 4317 and 4318

**What you learned**: The central collector is pre-deployed in the shared `observability-demo` namespace by GitOps. Your task is to create the per-namespace sidecar collector and Instrumentation CR in your own namespace, then wire the application into that pipeline.

=== Troubleshooting

**Issue**: `opentelemetrycollectors` CRD not found

**Solution**:
. Check if the operator CSV is installed: `oc get csv -n openshift-operators | grep opentelemetry`
. Check the subscription: `oc get subscription opentelemetry-product -n openshift-operators`
. Allow 2-3 minutes for the operator to register CRDs after installation

**Issue**: Central collector pods are not Running

**Solution**:
. Describe the CR: `oc describe opentelemetrycollector central-collector -n observability-demo`
. Check pod events: `oc get events -n observability-demo --sort-by=.lastTimestamp | tail -20`
. Confirm Tempo and Prometheus services are resolvable from `observability-demo`

== Exercise 2: Create the sidecar collector in your namespace

You will deploy a sidecar-mode `OpenTelemetryCollector` CR in your `{user}-observability-demo` namespace. When this CR exists, the OpenTelemetry Operator automatically injects a sidecar container into any pod in the namespace that carries the annotation `sidecar.opentelemetry.io/inject: "sidecar"`.

=== Steps

. Verify the required ServiceAccount is present in your namespace:
+
[source,bash]
----
oc get serviceaccount otel-collector-sidecar \
  -n {user}-observability-demo
----
+
This ServiceAccount was pre-created for your namespace with the RBAC permissions needed by the `k8sattributes` and `resourcedetection` processors (read access to pods, namespaces, and nodes).
+
.Expected output
[source,text]
----
NAME                    SECRETS   AGE
otel-collector-sidecar  0         1h
----

. Create the sidecar OpenTelemetryCollector CR:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: sidecar
  namespace: {user}-observability-demo
spec:
  mode: sidecar
  serviceAccount: otel-collector-sidecar
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15
      resourcedetection:
        detectors: [openshift]
        timeout: 2s
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
      batch:
        timeout: 10s
        send_batch_size: 1024
    exporters:
      otlp:
        endpoint: central-collector-collector.observability-demo.svc:4317
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
EOF
----

. Verify the CR was accepted:
+
[source,bash]
----
oc get opentelemetrycollector sidecar -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME     MODE    VERSION
sidecar  sidecar 0.140.0-2
----
+
In `sidecar` mode, the operator does not create a standalone `Deployment`. Instead it stores the container spec and injects it into pods on demand when the annotation is detected.

=== Understand the pipeline

The sidecar carries **all three signal types** over the same processor chain:

[source,text]
----
              otlp receiver  (localhost:4317 gRPC / localhost:4318 HTTP)
                    |
         memory_limiter  -- drop if pod memory > 75%
                    |
         resourcedetection -- add k8s.cluster.name, cloud.platform
                    |
         k8sattributes   -- add k8s.pod.name, k8s.deployment.name,
                            k8s.namespace.name, k8s.pod.uid
                    |
              batch        -- buffer and flush (max 10s / 1024 records)
                    |
         otlp exporter  -> central-collector-collector.observability-demo.svc:4317
                            (gRPC, insecure – TLS terminated at central)
----

The sidecar does **not** route signals to different destinations—that responsibility belongs to the central collector. All signals arrive at the central on a single gRPC stream.

=== Verify

* ✓ `otel-collector-sidecar` ServiceAccount exists in `{user}-observability-demo`
* ✓ `sidecar` OpenTelemetryCollector CR exists in `sidecar` mode
* ✓ CR configuration includes all three pipelines (traces, metrics, logs)
* ✓ Exporter endpoint points to `central-collector-collector.observability-demo.svc:4317`

=== Troubleshooting

**Issue**: `oc apply` returns `forbidden` when creating the CR

**Solution**:
. Confirm you are logged in as the correct user: `oc whoami`
. Confirm your project is set: `oc project`
. Verify you have create access: `oc auth can-i create opentelemetrycollectors -n {user}-observability-demo`

**Issue**: ServiceAccount `otel-collector-sidecar` not found

**Solution**:
. Check available service accounts: `oc get sa -n {user}-observability-demo`
. Contact the workshop facilitator—the ServiceAccount should be pre-provisioned as part of the workshop setup

== Exercise 3: Create the Instrumentation CR in your namespace

The `Instrumentation` CR is a template that tells the OpenTelemetry Operator how to configure the auto-instrumentation agent init-container when injecting into pods. You need one per namespace. In this exercise you create it for `{user}-observability-demo` — it will be used in Exercise 7 when you enable Python auto-instrumentation on the **notifier** service.

=== Steps

. Create the Instrumentation CR:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: my-instrumentation
  namespace: {user}-observability-demo
spec:
  exporter:
    endpoint: http://localhost:4318
  sampler:
    type: parentbased_traceidratio
    argument: "1.0"
  propagators:
    - tracecontext
    - baggage
  python:
    env:
      - name: OTEL_EXPORTER_OTLP_PROTOCOL
        value: http/protobuf
EOF
----
+
Key fields explained:
+
* `spec.exporter.endpoint: http://localhost:4318` — the auto-instrumented process sends telemetry to the sidecar on localhost (both containers live in the same pod)
* `spec.sampler.type: parentbased_traceidratio` with `argument: "1.0"` — 100% sampling, suitable for a workshop
* `spec.propagators: [tracecontext, baggage]` — W3C Trace Context headers so the incoming `traceparent` from the Go backend is read and spans are linked into the existing trace
* `spec.python.env OTEL_EXPORTER_OTLP_PROTOCOL: http/protobuf` — forces the Python SDK to use HTTP/protobuf rather than gRPC, matching the sidecar receiver on port 4318

. Verify the CR was accepted:
+
[source,bash]
----
oc get instrumentation my-instrumentation -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME                 AGE
my-instrumentation   10s
----

=== Verify

* ✓ `my-instrumentation` Instrumentation CR exists in `{user}-observability-demo`
* ✓ `spec.exporter.endpoint` is `http://localhost:4318`

**What you learned**: The `Instrumentation` CR is a per-namespace configuration template. Unlike the sidecar CR (which provides a container spec merged into application pods), the Instrumentation CR is referenced by pods via the `instrumentation.opentelemetry.io/inject-<language>` annotation — the Operator reads it and injects a language-specific agent init-container plus the necessary environment variables with no application changes required.

=== Troubleshooting

**Issue**: `oc apply` returns a validation error about `apiVersion`

**Solution**:
. The `Instrumentation` CRD uses `opentelemetry.io/v1alpha1`, not `v1beta1`. Confirm: `oc api-resources | grep instrumentation`

== Exercise 4: Enable OpenTelemetry on the applications

Now you'll activate the OpenTelemetry SDK in all three services by applying the `enable-otel.yaml` patch file. This adds the sidecar injection annotation and the SDK environment variables to each deployment without rebuilding container images.

=== Steps

. Set your namespace as a shell variable:
+
[source,bash]
----
NAMESPACE="{user}-observability-demo"
----

. Review what `enable-otel.yaml` will change:
+
TIP: Open the *Source Code* tab in the workshop application and select `enable-otel.yaml` to browse the full patch file.
+
Or inspect the key fields directly from GitHub:
+
[source,bash]
----
curl -sL https://raw.githubusercontent.com/cldmnky/observability-workshop/main/src/enable-otel.yaml \
  | grep -E "OTEL_|inject:|serviceAccountName:"
----
+
You will see three key changes per deployment:
+
* `sidecar.opentelemetry.io/inject: "sidecar"` — triggers sidecar injection by matching the CR name you created
* `OTEL_ENABLED: "true"` — activates the Go SDK
* `OTEL_EXPORTER_OTLP_ENDPOINT: "http://localhost:4318"` — routes telemetry to the injected sidecar

. Apply the patch to your namespace:
+
[source,bash]
----
curl -sL https://raw.githubusercontent.com/cldmnky/observability-workshop/main/src/enable-otel.yaml \
  | sed "s/__NAMESPACE__/${NAMESPACE}/g" \
  | oc apply -f -
----
+
.Expected output
[source,text]
----
deployment.apps/frontend configured
deployment.apps/backend configured
deployment.apps/database configured
----

. Monitor the rolling restart:
+
[source,bash]
----
oc rollout status deployment/frontend deployment/backend deployment/database \
  -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
deployment "frontend" successfully rolled out
deployment "backend" successfully rolled out
deployment "database" successfully rolled out
----

. Verify sidecar injection occurred:
+
[source,bash]
----
oc get pods -n {user}-observability-demo \
  -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'
----
+
.Expected output
[source,text]
----
NAME                        CONTAINERS
frontend-xxxxx              frontend, otc-container
backend-xxxxx               backend, otc-container
database-xxxxx              database, otc-container
----
+
The `otc-container` is the injected OpenTelemetry Collector sidecar. Each pod now has two containers: the application and its dedicated collector.

. Confirm the sidecar container is running and pipelines have started:
+
[source,bash]
----
oc logs -n {user}-observability-demo \
  -l app=frontend -c otc-container | tail -10
----
+
.Expected output (excerpt)
[source,text]
----
Everything is ready. Begin running and processing data.
Pipeline started (traces).
Pipeline started (metrics).
Pipeline started (logs).
----

. Generate application traffic to produce telemetry:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 30); do
  curl -sk -o /dev/null "https://$FRONTEND_URL/"
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/new-note" \
    -d "title=Test+$i&content=OTel+test"
  sleep 1
done
echo "Traffic generation complete"
----
+
This sends 30 iterations of mixed GET and POST requests, generating spans at each hop through frontend → backend → database.

=== Verify

* ✓ `enable-otel.yaml` applied cleanly to all three deployments
* ✓ All three deployments rolled out successfully
* ✓ Each pod has two containers: the application and `otc-container` (sidecar)
* ✓ Sidecar logs show all three pipelines started
* ✓ Traffic generated against the frontend route

**What you learned**: The OpenTelemetry Operator's sidecar injection mechanism transforms a running pod by adding a pre-configured collector container, without rebuilding the image. The annotation `sidecar.opentelemetry.io/inject: "sidecar"` tells the operator to use the `sidecar` CR you created in the previous exercise as the container specification.

=== Troubleshooting

**Issue**: Pods show only 1 container after rollout

**Solution**:
. Check that the annotation was applied: `oc get deployment frontend -n {user}-observability-demo -o jsonpath='{.spec.template.metadata.annotations}'`
. Verify the sidecar CR exists: `oc get opentelemetrycollector sidecar -n {user}-observability-demo`
. Check operator logs for injection errors: `oc logs -n openshift-operators -l app.kubernetes.io/name=opentelemetry-operator | grep -i inject`

**Issue**: Sidecar container is in `CrashLoopBackOff`

**Solution**:
. View sidecar logs: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container`
. Common cause: ServiceAccount lacks permissions for `k8sattributes`—verify the ServiceAccount RBAC
. Check central collector reachability: `oc exec -n {user}-observability-demo deployment/frontend -c otc-container -- wget -qO- http://central-collector-collector.observability-demo.svc:13133/`

**Issue**: `sed` substitution produces empty output

**Solution**:
. Verify the variable is set: `echo $NAMESPACE`
. Test the full pipeline: `curl -sL https://raw.githubusercontent.com/cldmnky/observability-workshop/main/src/enable-otel.yaml | sed "s/__NAMESPACE__/${NAMESPACE}/g" | head -20`
. If `curl` fails, check network connectivity from the terminal pod

== Exercise 5: View live traces in Observe → Traces

With traffic flowing and telemetry active, you can now view real traces from your application in the OpenShift console.

=== Steps

. Navigate to **Observe** → **Traces** in the OpenShift console.

. Set the search parameters:
+
* **Namespace**: `{user}-observability-demo`
* **Time range**: Last 5 minutes
+
Click **Search** or the refresh icon.
+
The scatter plot should now show individual points, one per trace, with y-axis showing duration in milliseconds.

. Identify slow traces:
+
Look at the scatter plot for points significantly higher on the y-axis than the main cluster.
+
Click one of the higher points to open the trace detail view.

. Explore the trace waterfall:
+
The waterfall shows a horizontal bar for each span, indented to reflect parent-child relationships:
+
[source,text]
----
frontend: GET /new-note                  [==============================] 95ms
  backend: POST /api/notes               [========================] 80ms
    database: POST /api/events           [==================] 60ms
----
+
Each bar length represents duration. The widest bar is the slowest operation in the request chain.

. Click a span to expand its attributes:
+
Select the `database` span. You will see attributes populated by both the application and the sidecar processors:
+
* **Application attributes**: `http.method`, `http.status_code`, `http.route`
* **Kubernetes attributes** (added by sidecar `k8sattributes`): `k8s.pod.name`, `k8s.deployment.name`, `k8s.namespace.name`
* **Resource attributes** (added by `resourcedetection`): `cloud.platform`, `k8s.cluster.name`

. Use TraceQL to find slow database spans:
+
Click **Show query** and enter:
+
[source,text]
----
{ resource.service.name = "database" && duration > 50ms }
----
+
This filters to only traces where the database service had at least one span exceeding 50ms.

. Find error traces:
+
[source,text]
----
{ status = error }
----
+
If any requests returned HTTP errors, this will surface the traces where something went wrong, with all spans visible for root cause analysis.

. Correlate a trace with logs:
+
Note the **Trace ID** shown at the top of a trace detail view (a 32-character hex string).
+
Navigate to **Observe** → **Logs**.
+
Query for log lines containing that trace ID:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} |= "<your-trace-id>"
----
+
Because the application uses `otelslog.NewHandler()`, every structured log line emitted during a traced request carries the active trace ID as a log field. This lets you move directly from a slow span to the exact log lines emitted during that span.

=== Verify

* ✓ Traces appear in **Observe → Traces** for namespace `{user}-observability-demo`
* ✓ Trace waterfall shows spans from all three services (frontend, backend, database)
* ✓ Span attributes include Kubernetes metadata (`k8s.pod.name`, `k8s.deployment.name`)
* ✓ TraceQL query `{ resource.service.name = "database" }` returns results
* ✓ A trace ID from the trace view can be found in **Observe → Logs**

**What you learned**: The two-tier collector architecture enriches telemetry with Kubernetes context at the sidecar level before forwarding to the central collector. Every trace, metric, and log automatically carries pod, deployment, and cluster metadata—no manual attribute configuration in application code required.

=== Troubleshooting

**Issue**: No traces appear after generating traffic

**Solution**:
. Verify traffic reached the app: `oc logs -n {user}-observability-demo deployment/frontend -c frontend | tail -10`
. Check sidecar export errors: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container | grep -i "error\|export"`
. Verify central collector received spans: `oc logs -n observability-demo -l app.kubernetes.io/name=central-collector-collector | grep -i "error\|span"`
. Confirm Tempo is healthy: `oc get tempostack -n openshift-tempo-operator`
. Expand the trace search time range to the last 15 minutes

**Issue**: Traces appear but show spans from only one service

**Solution**:
. Verify all three deployments have sidecars: `oc get pods -n {user}-observability-demo -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'`
. Confirm `OTEL_ENABLED` is set on all deployments: `oc set env deployment/backend -n {user}-observability-demo --list | grep OTEL`

**Issue**: Spans show no Kubernetes metadata attributes

**Solution**:
. Check ServiceAccount permissions: `oc auth can-i get pods --as=system:serviceaccount:{user}-observability-demo:otel-collector-sidecar`
. Check `k8sattributes` errors in sidecar logs: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container | grep -i k8s`

== Exercise 6: Explore the central collector pipeline

The central collector in `observability-demo` receives telemetry from all sidecar collectors and routes it to three backends. Inspect the configuration and verify each export path.

=== Steps

. View the central collector configuration:
+
[source,bash]
----
oc get opentelemetrycollector central-collector \
  -n observability-demo \
  -o jsonpath='{.spec.config}' | yq .
----
+
Note the four export destinations:
+
* `otlp/tempo` → `tempo-tempo-distributor.openshift-tempo-operator.svc:4317` — traces, TLS + bearer token auth + `X-Scope-OrgID: dev` header
* `prometheusremotewrite` → COO Prometheus `/api/v1/write` — application metrics pushed from pods
* `prometheusremotewrite` (via `traces/spanmetrics` pipeline) → same endpoint — span-derived RED metrics
* `otlphttp/logs` → LokiStack gateway at `openshift-logging` — logs, OTLP/HTTP JSON + bearer token + service CA TLS

. Inspect the spanmetrics connector:
+
[source,bash]
----
oc get opentelemetrycollector central-collector \
  -n observability-demo \
  -o jsonpath='{.spec.config}' | yq '.connectors.spanmetrics'
----
+
The `spanmetrics` connector acts as both an exporter (receiving spans from the traces pipeline) and a receiver (producing metric records for the `traces/spanmetrics` pipeline). It generates latency histograms and call-count counters for every `service.name` + `span.name` combination automatically.

. Query the generated span metrics in Prometheus:
+
Navigate to **Observe** → **Metrics** and enter:
+
[source,promql]
----
sum(rate(traces_spanmetrics_calls_total{service_name="frontend"}[5m])) by (span_name)
----
+
This shows the request rate per operation for the frontend service—derived solely from traces, with no Prometheus client library code in the application.
+
To see the p95 latency for backend operations:
+
[source,promql]
----
histogram_quantile(0.95,
  sum(rate(traces_spanmetrics_latency_bucket{service_name="backend"}[5m]))
  by (span_name, le)
)
----

. Check the central collector's own health endpoint:
+
[source,bash]
----
oc exec -n observability-demo \
  deployment/central-collector-collector -- \
  wget -qO- http://localhost:13133/
----
+
.Expected output
[source,text]
----
Server available, lifetime uptime xxx seconds
----

. Understand the logs pipeline:
+
The central collector runs a dedicated logs pipeline that maps OTEL resource attributes to the label keys expected by the LokiStack `openshift-logging` multi-tenancy gateway:
+
[source,yaml]
----
processors:
  resource/logs:            # <1>
    attributes:
      - {key: kubernetes.namespace_name, from_attribute: k8s.namespace.name, action: upsert}
      - {key: kubernetes.pod_name,       from_attribute: k8s.pod.name,       action: upsert}
      - {key: kubernetes.container_name, from_attribute: k8s.container.name, action: upsert}
      - {key: log_type, value: application, action: upsert}
  transform/logs:           # <2>
    log_statements:
      - context: log
        statements:
          - set(attributes["level"], ConvertCase(severity_text, "lower"))
exporters:
  otlphttp/logs:            # <3>
    endpoint: https://logging-loki-gateway-http.openshift-logging.svc.cluster.local:8080/api/logs/v1/application/otlp
    encoding: json
    tls:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
    auth:
      authenticator: bearertokenauth  # <4>
----
<1> LokiStack in `openshift-logging` tenancy mode requires `kubernetes.namespace_name` (not `k8s.namespace.name`) for tenant routing.
<2> Derives a `level` field from OTEL `severity_text` (lowercased) for log filtering in the Loki UI.
<3> Sends log records as OTLP/HTTP JSON to the LokiStack gateway's application tenant endpoint.
<4> Uses the pod's service account token for bearer auth — the central collector SA has `loki.grafana.com/application` create rights via ClusterRoleBinding.
+
Verify a log record arrived in Loki by navigating to **Observe** → **Logs** in the OpenShift console and querying:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} | json
----
+
You should see structured log records from `frontend`, `backend`, and `database`, each containing `traceID` and `spanID` fields that link them to the traces you viewed in Exercise 5.

=== Verify

* ✓ Central collector configuration shows `otlp/tempo`, `prometheusremotewrite`, and `otlphttp/logs` exporters
* ✓ `spanmetrics` connector configuration is visible
* ✓ `traces_spanmetrics_calls_total` metric exists in Prometheus for your services
* ✓ `traces_spanmetrics_latency_bucket` histogram is queryable for p95 latency
* ✓ Central collector health endpoint returns `Server available`
* ✓ **Observe → Logs** shows structured log records from `{user}-observability-demo` with `traceID` fields

**What you learned**: The central collector is a fanout hub — one OTLP receiver, four exporters. Each signal type is independently processed and routed: traces go to Tempo (with TLS and multi-tenancy headers), metrics are remote-written to COO Prometheus (which must have `enableRemoteWriteReceiver: true`), span-derived RED metrics follow the same path via the `spanmetrics` connector, and logs are attribute-remapped and forwarded to the LokiStack application tenant using service-account bearer auth.

== Exercise 7: Zero-code Python auto-instrumentation

The workshop application now includes a fourth service: **notifier**, a Python/FastAPI microservice. The backend calls notifier after every note create, update, or delete operation. The notifier records the event in the database service.

Open `src/notifier/app.py` and notice what is absent: there are **no OpenTelemetry imports** of any kind. The file contains only FastAPI route handlers and an httpx call. Yet by the end of this exercise, full traces—including spans for every notifier HTTP call—will appear in the trace waterfall.

TIP: Open the *Source Code* tab in the workshop application and select `notifier/app.py` to compare it with the Go services.

This demonstrates the difference between the **manual SDK approach** used by the Go services and the **zero-code auto-instrumentation** provided by the OpenTelemetry Operator.

[options="header",cols="30,35,35"]
|===
|Service |Language |Instrumentation method
|frontend |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|backend |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|database |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|**notifier** |**Python** |**Zero-code: OTel Operator init-container injection**
|===

=== Step 1: Observe the missing notifier span

Before enabling auto-instrumentation, generate traffic and look at a trace waterfall.

. Generate several note-creation requests:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 10); do
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/api/notes" \
    -H "Content-Type: application/json" \
    -d "{\"title\":\"Auto-instrumentation test $i\",\"content\":\"Exercise 6\"}"
  sleep 1
done
----

. Open **Observe** → **Traces** in the OpenShift console, set namespace to `{user}-observability-demo`, and click a recent trace.
+
The waterfall will show three hops: `frontend → backend → database`. The backend called notifier, but no notifier span appears because the Python process emits no telemetry without an agent.

=== Step 2: Inspect the Instrumentation CR you created

The OpenTelemetry Operator reads the `Instrumentation` CR you created in Exercise 3 as a template for agent injection.

. Verify it exists in your namespace:
+
[source,bash]
----
oc get instrumentation my-instrumentation \
  -n {user}-observability-demo -o yaml
----
+
Key fields:
+
[source,yaml]
----
spec:
  exporter:
    endpoint: http://localhost:4318   # <1>
  propagators:
    - tracecontext
    - baggage                          # <2>
  sampler:
    type: parentbased_traceidratio
    argument: "1.0"                    # <3>
  python:
    env:
      - name: OTEL_EXPORTER_OTLP_PROTOCOL
        value: http/protobuf            # <4>
----
+
<1> The auto-instrumented process sends telemetry to the sidecar on `localhost:4318`.
<2> W3C Trace Context propagators ensure the incoming `traceparent` header from the backend is read, linking the notifier span to the existing trace.
<3> 100% sampling is appropriate for this workshop environment.
<4> Forces the Python SDK to use HTTP/protobuf rather than gRPC, matching the sidecar receiver.


=== Step 3: Annotate the notifier deployment

A single `oc patch` command adds the two annotations that trigger both sidecar injection (same as the Go services) and Python agent injection (new for notifier).

. Patch the notifier deployment:
+
[source,bash]
----
oc patch deployment notifier \
  -n {user}-observability-demo \
  --type=json \
  -p='[
    {
      "op": "add",
      "path": "/spec/template/metadata/annotations",
      "value": {
        "sidecar.opentelemetry.io/inject": "sidecar",
        "instrumentation.opentelemetry.io/inject-python": "my-instrumentation"
      }
    },
    {
      "op": "add",
      "path": "/spec/template/spec/serviceAccountName",
      "value": "otel-collector-sidecar"
    }
  ]'
----
+
The annotation value `my-instrumentation` references the Instrumentation CR you created in Exercise 3 in the same namespace.
+
When this annotated pod is scheduled, the OpenTelemetry Operator's mutating admission webhook injects an init container that downloads `opentelemetry-distro` and `opentelemetry-instrumentation-fastapi` into a shared volume. The Python process picks them up via `PYTHONPATH` and a configurator hook—no code changes or image rebuilds required.

. Watch the rollout:
+
[source,bash]
----
oc rollout status deployment/notifier -n {user}-observability-demo
----

. Confirm the pod now has two containers (application + sidecar):
+
[source,bash]
----
oc get pods -n {user}-observability-demo \
  -l app=notifier \
  -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'
----
+
.Expected output
[source,text]
----
NAME                        CONTAINERS
notifier-xxxxx              notifier, otc-container
----

. Verify the Python SDK is active by checking notifier logs:
+
[source,bash]
----
oc logs -n {user}-observability-demo \
  -l app=notifier -c notifier | head -20
----
+
Look for OpenTelemetry bootstrap messages such as `Instrumenting FastAPI` or `OpenTelemetry SDK configured`.

=== Step 4: Generate traffic and observe the 4-hop trace

. Send another batch of note-creation requests:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 15); do
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/api/notes" \
    -H "Content-Type: application/json" \
    -d "{\"title\":\"Traced note $i\",\"content\":\"With notifier\"}"
  sleep 1
done
echo "Done"
----

. Return to **Observe** → **Traces** in the console. A note-creation trace will now show a fourth hop:
+
[source,text]
----
frontend: POST /api/notes                     [=====================================] 130ms
  backend: POST /api/notes                    [==============================] 105ms
    database: POST /notes                     [================] 50ms
    notifier: POST /notify                    [==========] 30ms
      notifier: POST /notify → database       [======] 18ms
----
+
The `notifier` spans are produced entirely by `opentelemetry-instrumentation-fastapi` and `opentelemetry-instrumentation-httpx`—**no code was changed in `app.py`**.

. Click on the `notifier: POST /notify` span and inspect its attributes:
+
* `http.method`, `http.status_code`, `http.route` — added by FastAPI auto-instrumentation
* `k8s.pod.name`, `k8s.deployment.name` — added by the sidecar `k8sattributes` processor
* `service.name: notifier` — set via the `OTEL_SERVICE_NAME` env var injected by the operator

. Notice that the `traceparent` context passed from backend to notifier is preserved correctly: the notifier root span appears as a **child** of the backend span, maintaining the single unified trace tree.

=== Compare: manual SDK vs auto-instrumentation

[options="header",cols="35,30,35"]
|===
|Characteristic |Go (manual SDK) |Python (auto-instrumentation)
|Code change required |Yes (`telemetry.Setup()`) |**No**
|Image rebuild required |Yes |**No**
|Activation mechanism |`OTEL_ENABLED=true` env var |Pod annotation
|Span granularity |Full control (custom spans) |Framework-level (HTTP in/out)
|Custom attributes |Easy (manual API calls) |Limited without code changes
|Best for |Apps with source access |Apps without source access or rapid onboarding
|===

Browse the two approaches side-by-side using the *Source Code* tab in the workshop application:

* `telemetry/telemetry.go` — shared OTEL SDK setup (Go)
* `backend/main.go` — manual SDK usage (Go)
* `notifier/app.py` — zero OTEL imports (Python)

=== Verify

* ✓ Notifier pod has two running containers: `notifier` and `otc-container`
* ✓ Notifier logs show OpenTelemetry SDK bootstrap messages
* ✓ Traces for note creation show four service hops: `frontend → backend → notifier → database`
* ✓ Notifier spans are children of the backend span (W3C Trace Context propagation works)
* ✓ `k8s.pod.name` and `k8s.deployment.name` attributes are present on notifier spans
* ✓ `app.py` was not modified at any point in this exercise

**What you learned**: The `Instrumentation` CR is a namespace-level template. A single annotation—`instrumentation.opentelemetry.io/inject-python`—causes the OpenTelemetry Operator's admission webhook to inject an init-container that downloads and configures the Python agent at pod start. No source changes, no image rebuilds, no SDK imports. The W3C Trace Context standard ensures the notifier's spans slot directly into the existing trace tree built by the Go services.

=== Troubleshooting

**Issue**: Notifier pod shows only one container after rollout

**Solution**:
. Verify the annotation is set: `oc get deployment notifier -n {user}-observability-demo -o jsonpath='{.spec.template.metadata.annotations}'`
. Confirm the sidecar CR exists: `oc get opentelemetrycollector sidecar -n {user}-observability-demo`
. Check operator logs: `oc logs -n openshift-operators -l app.kubernetes.io/name=opentelemetry-operator | grep -i "notifier\|inject\|python"`

**Issue**: Notifier spans appear but are disconnected from the main trace

**Solution**:
. Verify `propagators` includes `tracecontext`: `oc get instrumentation my-instrumentation -n {user}-observability-demo -o jsonpath='{.spec.propagators}'`
. Confirm `OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf` is injected: `oc exec -n {user}-observability-demo deployment/notifier -- env | grep OTEL`

**Issue**: No traces appear for the notifier after annotation

**Solution**:
. Check if the Python bootstrap ran: `oc logs -n {user}-observability-demo deployment/notifier -c notifier | grep -i opentelemetry`
. Verify the sidecar received data: `oc logs -n {user}-observability-demo deployment/notifier -c otc-container | grep -i "span\|error"`
. Ensure the Instrumentation CR exists in your namespace: `oc get instrumentation my-instrumentation -n {user}-observability-demo`

== Learning outcomes

By completing this module, you should now understand:

* ✓ The **sidecar-to-central** collector pattern decouples application-facing collection from backend-facing export
* ✓ A **sidecar OpenTelemetryCollector** in `sidecar` mode injects a container into annotated pods without any image changes
* ✓ The **`k8sattributes` and `resourcedetection` processors** automatically enrich telemetry with Kubernetes and infrastructure metadata
* ✓ Applying `enable-otel.yaml` simultaneously activates the Go SDK, sets the OTLP endpoint, and triggers sidecar injection
* ✓ The **`spanmetrics` connector** in the central collector generates RED metrics from traces, eliminating the need for a Prometheus client library
* ✓ **Auto-instrumentation** via the `Instrumentation` CR enables zero-code telemetry for Java, Python, and Node.js applications
* ✓ A **Python/FastAPI service** (notifier) can produce full traces—including W3C context propagation—with zero source-code changes

**Business impact**: You now have a complete three-signal observability pipeline for all four microservices. A single request to your application automatically produces:

* A **distributed trace** showing the exact call path and per-service timing
* **RED metrics** (rate, errors, duration) queryable in Prometheus—without any metrics code in the application
* **Structured log records** correlated to the trace via trace ID—without any manual log attribute configuration

This gives the platform engineering team full observability with minimal developer friction.

== Module summary

You activated the full OpenTelemetry telemetry pipeline for the workshop application—from sidecar collector creation through to live trace visualization in the OpenShift console.

**What you accomplished**:

* Verified the OpenTelemetry Operator and the pre-deployed central collector in `observability-demo`
* Created a sidecar `OpenTelemetryCollector` CR in your namespace — carries all three signals (traces, metrics, logs) to the central collector over a single gRPC connection
* Created an `Instrumentation` CR in your namespace — used as the agent injection template for Python auto-instrumentation
* Applied `enable-otel.yaml` to activate the Go SDK and trigger sidecar injection on the three Go deployments
* Analyzed live traces in **Observe → Traces**, identified span-level bottlenecks, and correlated spans with log records
* Explored the central collector's full routing: traces → Tempo, metrics + RED metrics → COO Prometheus remote write, logs → LokiStack application tenant
* Enabled zero-code Python auto-instrumentation on the **notifier** service using a single pod annotation
* Observed a **four-hop trace** (`frontend → backend → notifier → database`) with full context propagation across Go and Python services

**Key concepts mastered**:

* **Sidecar mode**: The operator injects the collector as a second container into annotated pods
* **k8sattributes and resourcedetection**: Processors that attach Kubernetes context metadata to all telemetry
* **Two-tier architecture**: Sidecar handles application-facing collection; central collector handles backend-facing export and routing
* **spanmetrics connector**: Generates Prometheus-queryable RED metrics automatically from trace spans
* **Auto-instrumentation**: `Instrumentation` CR + pod annotation enables agent injection for Java, Python, Node.js, and .NET without code changes
* **Cross-language tracing**: W3C Trace Context headers propagate correctly between Go (`otelhttp`) and Python (`opentelemetry-instrumentation-httpx`), forming a single unified trace tree

You have now completed the core modules of the OpenShift Observability Workshop. Continue to the conclusion to review key takeaways and next steps.
