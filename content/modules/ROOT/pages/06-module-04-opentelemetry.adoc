= Module 4: OpenTelemetry integration
:source-highlighter: rouge
:toc: macro
:toclevels: 1

In Module 3 you verified the Tempo tracing backend, enabled the Distributed Tracing UI plugin, and explored how the Go application is instrumented with the OpenTelemetry SDK. The telemetry pipeline was not yet active—the application was running silently.

In this module you will activate that pipeline end-to-end. You'll deploy a sidecar OpenTelemetry Collector in your namespace, enable the SDK on all three services, and observe live traces, metrics, and logs flowing from your application through the collector pipeline to Tempo, Prometheus, and Loki.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand the sidecar-to-central OpenTelemetry Collector architecture
* Create a sidecar `OpenTelemetryCollector` CR and configure its pipeline
* Enable OpenTelemetry on the workshop application using `enable-otel.yaml`
* Observe live traces in the **Observe → Traces** console UI
* Understand how the central collector fans out telemetry to Tempo, Prometheus, and Loki
* Enable zero-code Python auto-instrumentation on the **notifier** service using the `Instrumentation` CR
* Observe a four-hop trace spanning Go and Python services in a single waterfall

== Understanding the collector architecture

Before enabling telemetry, understand how data flows from the application to each observability backend.

=== Two-tier collector pattern

This workshop uses a two-tier OpenTelemetry Collector topology:

[source,text]
----
Application pods ({user}-observability-demo)
  +------------------------------------------+
  |  frontend                                |
  |  +----------+   OTLP (localhost:4318)    |
  |  | app      |---> sidecar collector      |
  |  +----------+                            |
  +------------------------------------------+
         |
         | OTLP gRPC (cluster DNS)
         v
  central-collector-collector.observability-demo.svc:4317
  +----------------------------------------------------------+
  |  Central collector (observability-demo namespace)        |
  |                                                          |
  |  Receivers:  otlp (4317, 4318)                           |
  |  Connectors: spanmetrics (traces -> RED metrics)         |
  |  Exporters:                                              |
  |   +-- otlp/tempo  -> tempo-tempo-distributor:4317        |
  |   +-- prometheusremotewrite -> COO Prometheus :9090      |
  |   +-- otlphttp/logs -> LokiStack gateway :8080           |
  +----------------------------------------------------------+
----

**Why two tiers?**

* **Sidecar collector**: Runs within the application pod, receives telemetry on `localhost:4318`, enriches it with Kubernetes pod metadata via the `k8sattributes` processor, and forwards to the central collector.
* **Central collector**: Runs as a `Deployment` in the shared `observability-demo` namespace with 2 replicas for resilience. Handles backend-specific protocols (TLS for Tempo, remote-write for Prometheus, OTLP/HTTP for Loki), applies the `spanmetrics` connector to generate RED metrics from traces, and holds the authentication tokens for each backend.

This pattern decouples application-facing collection (simple, low-overhead sidecar) from backend-facing export (complex, authenticated, shared).

=== Processors in the sidecar collector

The sidecar uses four processors in order:

[options="header",cols="25,75"]
|===
|Processor |Function
|`memory_limiter` |Prevents the collector from consuming more than 75% of available pod memory
|`resourcedetection` |Detects OpenShift infrastructure attributes (such as `k8s.cluster.name`) and adds them to all telemetry
|`k8sattributes` |Calls the Kubernetes API to attach pod name, namespace, deployment name, node name, and pod UID to every span, metric, and log record
|`batch` |Accumulates records before sending to reduce network overhead
|===

=== Span metrics connector in the central collector

The central collector uses a `spanmetrics` connector to generate **RED metrics** automatically from incoming traces:

* **Rate**: `traces_spanmetrics_calls_total` — request count per service and operation
* **Errors**: `traces_spanmetrics_calls_total{status.code="STATUS_CODE_ERROR"}` — error count
* **Duration**: `traces_spanmetrics_latency_bucket` — latency histogram with configurable buckets

These metrics are published to the COO-managed Prometheus instance via remote write, providing service-level latency histograms without any Prometheus client instrumentation in the application code.

== Exercise 1: Verify the OpenTelemetry operator and pre-deployed components

Before creating resources in your namespace, confirm the OpenTelemetry Operator and the shared `observability-demo` infrastructure are healthy.

=== Steps

. Verify the OpenTelemetry Operator pod is running:
+
[source,bash]
----
oc get pods -n openshift-operators \
  -l app.kubernetes.io/name=opentelemetry-operator
----
+
.Expected output
[source,text]
----
NAME                                          READY   STATUS    RESTARTS   AGE
opentelemetry-operator-controller-xxxxx       2/2     Running   0          1h
----

. Confirm the operator registered the CRDs:
+
[source,bash]
----
oc api-resources | grep opentelemetry
----
+
.Expected output
[source,text]
----
instrumentations              opentelemetry.io   true    Instrumentation
opentelemetrycollectors       opentelemetry.io   true    OpenTelemetryCollector
----

. Inspect the pre-deployed central collector:
+
[source,bash]
----
oc get opentelemetrycollector central-collector -n observability-demo
----
+
.Expected output
[source,text]
----
NAME               MODE         VERSION
central-collector  deployment   0.140.0-2
----

. Verify the central collector pods are running:
+
[source,bash]
----
oc get pods -n observability-demo \
  -l app.kubernetes.io/name=central-collector-collector
----
+
.Expected output
[source,text]
----
NAME                                      READY   STATUS    RESTARTS   AGE
central-collector-collector-xxxxx         1/1     Running   0          1h
central-collector-collector-xxxxx         1/1     Running   0          1h
----
+
Two replicas provide resilience for the shared collection endpoint.

. Inspect the central collector service (this is where sidecars forward telemetry):
+
[source,bash]
----
oc get svc central-collector-collector -n observability-demo
----
+
.Expected output
[source,text]
----
NAME                           TYPE        CLUSTER-IP      PORT(S)
central-collector-collector    ClusterIP   172.30.x.x      4317/TCP, 4318/TCP
----

. Inspect the pre-deployed Instrumentation CR:
+
[source,bash]
----
oc get instrumentation demo-instrumentation \
  -n observability-demo -o yaml
----
+
Note the key fields:
+
* `spec.exporter.endpoint: http://localhost:4318` — auto-instrumented pods send to the sidecar
* `spec.sampler.type: parentbased_traceidratio` with `argument: "1.0"` — sample 100% of traces (appropriate for a workshop environment)
* `spec.propagators: [tracecontext, baggage]` — uses W3C Trace Context headers for context propagation

=== Verify

* ✓ OpenTelemetry Operator pod is Running (2/2)
* ✓ `OpenTelemetryCollector` and `Instrumentation` CRDs are registered
* ✓ `central-collector` exists in `observability-demo` in deployment mode with 2 replicas
* ✓ `central-collector-collector` service exposes ports 4317 and 4318
* ✓ `demo-instrumentation` Instrumentation CR exists in `observability-demo`

**What you learned**: The central collector and the auto-instrumentation Instrumentation CR are pre-deployed in the shared `observability-demo` namespace by GitOps. Your task is to wire your application namespace into this shared infrastructure.

=== Troubleshooting

**Issue**: `opentelemetrycollectors` CRD not found

**Solution**:
. Check if the operator CSV is installed: `oc get csv -n openshift-operators | grep opentelemetry`
. Check the subscription: `oc get subscription opentelemetry-product -n openshift-operators`
. Allow 2-3 minutes for the operator to register CRDs after installation

**Issue**: Central collector pods are not Running

**Solution**:
. Describe the CR: `oc describe opentelemetrycollector central-collector -n observability-demo`
. Check pod events: `oc get events -n observability-demo --sort-by=.lastTimestamp | tail -20`
. Confirm Tempo and Prometheus services are resolvable from `observability-demo`

== Exercise 2: Create the sidecar collector in your namespace

You will deploy a sidecar-mode `OpenTelemetryCollector` CR in your `{user}-observability-demo` namespace. When this CR exists, the OpenTelemetry Operator automatically injects a sidecar container into any pod in the namespace that carries the annotation `sidecar.opentelemetry.io/inject: "sidecar"`.

=== Steps

. Verify the required ServiceAccount is present in your namespace:
+
[source,bash]
----
oc get serviceaccount otel-collector-sidecar \
  -n {user}-observability-demo
----
+
This ServiceAccount was pre-created for your namespace with the RBAC permissions needed by the `k8sattributes` and `resourcedetection` processors (read access to pods, namespaces, and nodes).
+
.Expected output
[source,text]
----
NAME                    SECRETS   AGE
otel-collector-sidecar  0         1h
----

. Create the sidecar OpenTelemetryCollector CR:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: sidecar
  namespace: {user}-observability-demo
spec:
  mode: sidecar
  serviceAccount: otel-collector-sidecar
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15
      resourcedetection:
        detectors: [openshift]
        timeout: 2s
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
      batch:
        timeout: 10s
        send_batch_size: 1024
    exporters:
      otlp:
        endpoint: central-collector-collector.observability-demo.svc:4317
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, k8sattributes, batch]
          exporters: [otlp]
EOF
----

. Verify the CR was accepted:
+
[source,bash]
----
oc get opentelemetrycollector sidecar -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
NAME     MODE    VERSION
sidecar  sidecar 0.140.0-2
----
+
In `sidecar` mode, the operator does not create a standalone `Deployment`. Instead it stores the container spec and injects it into pods on demand when the annotation is detected.

=== Understand the pipeline

The sidecar pipeline has three signal paths—traces, metrics, and logs—all using the same processor chain:

[source,text]
----
otlp receiver (localhost:4318)
  -> memory_limiter  (drop data if memory pressure)
  -> resourcedetection (add cluster.name)
  -> k8sattributes  (add pod/namespace/deployment metadata)
  -> batch          (buffer and flush)
  -> otlp exporter  (forward to central-collector:4317)
----

All three signals reach the central collector over a single gRPC connection, which then routes each to its appropriate backend.

=== Verify

* ✓ `otel-collector-sidecar` ServiceAccount exists in `{user}-observability-demo`
* ✓ `sidecar` OpenTelemetryCollector CR exists in `sidecar` mode
* ✓ CR configuration includes all three pipelines (traces, metrics, logs)
* ✓ Exporter endpoint points to `central-collector-collector.observability-demo.svc:4317`

=== Troubleshooting

**Issue**: `oc apply` returns `forbidden` when creating the CR

**Solution**:
. Confirm you are logged in as the correct user: `oc whoami`
. Confirm your project is set: `oc project`
. Verify you have create access: `oc auth can-i create opentelemetrycollectors -n {user}-observability-demo`

**Issue**: ServiceAccount `otel-collector-sidecar` not found

**Solution**:
. Check available service accounts: `oc get sa -n {user}-observability-demo`
. Contact the workshop facilitator—the ServiceAccount should be pre-provisioned as part of the workshop setup

== Exercise 3: Enable OpenTelemetry on the applications

Now you'll activate the OpenTelemetry SDK in all three services by applying the `enable-otel.yaml` patch file. This adds the sidecar injection annotation and the SDK environment variables to each deployment without rebuilding container images.

=== Steps

. Set your namespace as a shell variable:
+
[source,bash]
----
NAMESPACE={user}-observability-demo
----

. Review what `enable-otel.yaml` will change:
+
TIP: Open the *Source Code* tab in the workshop application and select `enable-otel.yaml` to browse the full patch file.
+
Or grep for the key fields directly:
+
[source,bash]
----
grep -E "OTEL_|inject:|serviceAccountName:" src/enable-otel.yaml
----
+
You will see three key changes per deployment:
+
* `sidecar.opentelemetry.io/inject: "sidecar"` — triggers sidecar injection by matching the CR name you created
* `OTEL_ENABLED: "true"` — activates the Go SDK
* `OTEL_EXPORTER_OTLP_ENDPOINT: "http://localhost:4318"` — routes telemetry to the injected sidecar

. Apply the patch to your namespace:
+
[source,bash]
----
sed "s/__NAMESPACE__/$NAMESPACE/g" src/enable-otel.yaml | oc apply -f -
----
+
.Expected output
[source,text]
----
deployment.apps/frontend configured
deployment.apps/backend configured
deployment.apps/database configured
----

. Monitor the rolling restart:
+
[source,bash]
----
oc rollout status deployment/frontend deployment/backend deployment/database \
  -n {user}-observability-demo
----
+
.Expected output
[source,text]
----
deployment "frontend" successfully rolled out
deployment "backend" successfully rolled out
deployment "database" successfully rolled out
----

. Verify sidecar injection occurred:
+
[source,bash]
----
oc get pods -n {user}-observability-demo \
  -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'
----
+
.Expected output
[source,text]
----
NAME                        CONTAINERS
frontend-xxxxx              frontend, otc-container
backend-xxxxx               backend, otc-container
database-xxxxx              database, otc-container
----
+
The `otc-container` is the injected OpenTelemetry Collector sidecar. Each pod now has two containers: the application and its dedicated collector.

. Confirm the sidecar container is running and pipelines have started:
+
[source,bash]
----
oc logs -n {user}-observability-demo \
  -l app=frontend -c otc-container | tail -10
----
+
.Expected output (excerpt)
[source,text]
----
Everything is ready. Begin running and processing data.
Pipeline started (traces).
Pipeline started (metrics).
Pipeline started (logs).
----

. Generate application traffic to produce telemetry:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 30); do
  curl -sk -o /dev/null "https://$FRONTEND_URL/"
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/new-note" \
    -d "title=Test+$i&content=OTel+test"
  sleep 1
done
echo "Traffic generation complete"
----
+
This sends 30 iterations of mixed GET and POST requests, generating spans at each hop through frontend → backend → database.

=== Verify

* ✓ `enable-otel.yaml` applied cleanly to all three deployments
* ✓ All three deployments rolled out successfully
* ✓ Each pod has two containers: the application and `otc-container` (sidecar)
* ✓ Sidecar logs show all three pipelines started
* ✓ Traffic generated against the frontend route

**What you learned**: The OpenTelemetry Operator's sidecar injection mechanism transforms a running pod by adding a pre-configured collector container, without rebuilding the image. The annotation `sidecar.opentelemetry.io/inject: "sidecar"` tells the operator to use the `sidecar` CR you created in the previous exercise as the container specification.

=== Troubleshooting

**Issue**: Pods show only 1 container after rollout

**Solution**:
. Check that the annotation was applied: `oc get deployment frontend -n {user}-observability-demo -o jsonpath='{.spec.template.metadata.annotations}'`
. Verify the sidecar CR exists: `oc get opentelemetrycollector sidecar -n {user}-observability-demo`
. Check operator logs for injection errors: `oc logs -n openshift-operators -l app.kubernetes.io/name=opentelemetry-operator | grep -i inject`

**Issue**: Sidecar container is in `CrashLoopBackOff`

**Solution**:
. View sidecar logs: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container`
. Common cause: ServiceAccount lacks permissions for `k8sattributes`—verify the ServiceAccount RBAC
. Check central collector reachability: `oc exec -n {user}-observability-demo deployment/frontend -c otc-container -- wget -qO- http://central-collector-collector.observability-demo.svc:13133/`

**Issue**: `sed` substitution produces empty output

**Solution**:
. Verify the variable is set: `echo $NAMESPACE`
. Confirm `enable-otel.yaml` is in the `src/` directory: `ls src/enable-otel.yaml`
. Test the substitution: `sed "s/__NAMESPACE__/$NAMESPACE/g" src/enable-otel.yaml | head -10`

== Exercise 4: View live traces in Observe → Traces

With traffic flowing and telemetry active, you can now view real traces from your application in the OpenShift console.

=== Steps

. Navigate to **Observe** → **Traces** in the OpenShift console.

. Set the search parameters:
+
* **Namespace**: `{user}-observability-demo`
* **Time range**: Last 5 minutes
+
Click **Search** or the refresh icon.
+
The scatter plot should now show individual points, one per trace, with y-axis showing duration in milliseconds.

. Identify slow traces:
+
Look at the scatter plot for points significantly higher on the y-axis than the main cluster.
+
Click one of the higher points to open the trace detail view.

. Explore the trace waterfall:
+
The waterfall shows a horizontal bar for each span, indented to reflect parent-child relationships:
+
[source,text]
----
frontend: GET /new-note                  [==============================] 95ms
  backend: POST /api/notes               [========================] 80ms
    database: POST /api/events           [==================] 60ms
----
+
Each bar length represents duration. The widest bar is the slowest operation in the request chain.

. Click a span to expand its attributes:
+
Select the `database` span. You will see attributes populated by both the application and the sidecar processors:
+
* **Application attributes**: `http.method`, `http.status_code`, `http.route`
* **Kubernetes attributes** (added by sidecar `k8sattributes`): `k8s.pod.name`, `k8s.deployment.name`, `k8s.namespace.name`
* **Resource attributes** (added by `resourcedetection`): `cloud.platform`, `k8s.cluster.name`

. Use TraceQL to find slow database spans:
+
Click **Show query** and enter:
+
[source,text]
----
{ resource.service.name = "database" && duration > 50ms }
----
+
This filters to only traces where the database service had at least one span exceeding 50ms.

. Find error traces:
+
[source,text]
----
{ status = error }
----
+
If any requests returned HTTP errors, this will surface the traces where something went wrong, with all spans visible for root cause analysis.

. Correlate a trace with logs:
+
Note the **Trace ID** shown at the top of a trace detail view (a 32-character hex string).
+
Navigate to **Observe** → **Logs**.
+
Query for log lines containing that trace ID:
+
[source,logql]
----
{kubernetes_namespace_name="{user}-observability-demo"} |= "<your-trace-id>"
----
+
Because the application uses `otelslog.NewHandler()`, every structured log line emitted during a traced request carries the active trace ID as a log field. This lets you move directly from a slow span to the exact log lines emitted during that span.

=== Verify

* ✓ Traces appear in **Observe → Traces** for namespace `{user}-observability-demo`
* ✓ Trace waterfall shows spans from all three services (frontend, backend, database)
* ✓ Span attributes include Kubernetes metadata (`k8s.pod.name`, `k8s.deployment.name`)
* ✓ TraceQL query `{ resource.service.name = "database" }` returns results
* ✓ A trace ID from the trace view can be found in **Observe → Logs**

**What you learned**: The two-tier collector architecture enriches telemetry with Kubernetes context at the sidecar level before forwarding to the central collector. Every trace, metric, and log automatically carries pod, deployment, and cluster metadata—no manual attribute configuration in application code required.

=== Troubleshooting

**Issue**: No traces appear after generating traffic

**Solution**:
. Verify traffic reached the app: `oc logs -n {user}-observability-demo deployment/frontend -c frontend | tail -10`
. Check sidecar export errors: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container | grep -i "error\|export"`
. Verify central collector received spans: `oc logs -n observability-demo -l app.kubernetes.io/name=central-collector-collector | grep -i "error\|span"`
. Confirm Tempo is healthy: `oc get tempostack -n openshift-tempo-operator`
. Expand the trace search time range to the last 15 minutes

**Issue**: Traces appear but show spans from only one service

**Solution**:
. Verify all three deployments have sidecars: `oc get pods -n {user}-observability-demo -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'`
. Confirm `OTEL_ENABLED` is set on all deployments: `oc set env deployment/backend -n {user}-observability-demo --list | grep OTEL`

**Issue**: Spans show no Kubernetes metadata attributes

**Solution**:
. Check ServiceAccount permissions: `oc auth can-i get pods --as=system:serviceaccount:{user}-observability-demo:otel-collector-sidecar`
. Check `k8sattributes` errors in sidecar logs: `oc logs -n {user}-observability-demo deployment/frontend -c otc-container | grep -i k8s`

== Exercise 5: Explore the central collector pipeline

The central collector in `observability-demo` receives telemetry from all sidecar collectors and routes it to three backends. Inspect the configuration and verify each export path.

=== Steps

. View the central collector configuration:
+
[source,bash]
----
oc get opentelemetrycollector central-collector \
  -n observability-demo \
  -o jsonpath='{.spec.config}' | yq .
----
+
Note the three export destinations:
+
* `otlp/tempo` → `tempo-tempo-distributor.openshift-tempo-operator.svc:4317` (traces, with TLS and bearer token auth)
* `prometheusremotewrite` → COO Prometheus remote-write endpoint (metrics and span metrics)
* Logs pipeline (uses `debug` exporter by default in this workshop)

. Inspect the spanmetrics connector:
+
[source,bash]
----
oc get opentelemetrycollector central-collector \
  -n observability-demo \
  -o jsonpath='{.spec.config}' | yq '.connectors.spanmetrics'
----
+
The `spanmetrics` connector acts as both an exporter (receiving spans from the traces pipeline) and a receiver (producing metric records for the `traces/spanmetrics` pipeline). It generates latency histograms and call-count counters for every `service.name` + `span.name` combination automatically.

. Query the generated span metrics in Prometheus:
+
Navigate to **Observe** → **Metrics** and enter:
+
[source,promql]
----
sum(rate(traces_spanmetrics_calls_total{service_name="frontend"}[5m])) by (span_name)
----
+
This shows the request rate per operation for the frontend service—derived solely from traces, with no Prometheus client library code in the application.
+
To see the p95 latency for backend operations:
+
[source,promql]
----
histogram_quantile(0.95,
  sum(rate(traces_spanmetrics_latency_bucket{service_name="backend"}[5m]))
  by (span_name, le)
)
----

. Check the central collector's own health endpoint:
+
[source,bash]
----
oc exec -n observability-demo \
  deployment/central-collector-collector -- \
  wget -qO- http://localhost:13133/
----
+
.Expected output
[source,text]
----
Server available, lifetime uptime xxx seconds
----

. Understand the full Loki export path:
+
A complete production LokiStack log export from the OpenTelemetry Collector requires a dedicated logs pipeline with the `otlphttp/logs` exporter, bearer token authentication against the LokiStack gateway, and attribute remapping. The following excerpt shows the required exporter and processor configuration, as documented in the link:https://docs.redhat.com/en/documentation/red_hat_build_of_opentelemetry/3.9/html/forwarding_telemetry_data/otel-forwarding-telemetry[Red Hat build of OpenTelemetry 3.9 forwarding documentation^]:
+
[source,yaml]
----
exporters:
  otlphttp/logs:
    endpoint: https://logging-loki-gateway-http.openshift-logging.svc.cluster.local:8080/api/logs/v1/application/otlp
    encoding: json
    tls:
      ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt"
    auth:
      authenticator: bearertokenauth
processors:
  resource:
    attributes:
      - key: kubernetes.namespace_name
        from_attribute: k8s.namespace.name
        action: upsert
      - key: kubernetes.pod_name
        from_attribute: k8s.pod.name
        action: upsert
      - key: log_type
        value: application
        action: upsert
----

=== Verify

* ✓ Central collector configuration shows `otlp/tempo`, `prometheusremotewrite`, and logs exporters
* ✓ `spanmetrics` connector configuration is visible
* ✓ `traces_spanmetrics_calls_total` metric exists in Prometheus for your services
* ✓ `traces_spanmetrics_latency_bucket` histogram is queryable for p95 latency
* ✓ Central collector health endpoint returns `Server available`

**What you learned**: The central collector implements a fanout pattern—one OTLP receiver, multiple exporters. The `spanmetrics` connector bridges traces and metrics, generating RED signals automatically. A single instrumentation pass captures traces, metrics, and logs simultaneously.

== Exercise 6: Zero-code Python auto-instrumentation

The workshop application now includes a fourth service: **notifier**, a Python/FastAPI microservice. The backend calls notifier after every note create, update, or delete operation. The notifier records the event in the database service.

Open `src/notifier/app.py` and notice what is absent: there are **no OpenTelemetry imports** of any kind. The file contains only FastAPI route handlers and an httpx call. Yet by the end of this exercise, full traces—including spans for every notifier HTTP call—will appear in the trace waterfall.

TIP: Open the *Source Code* tab in the workshop application and select `notifier/app.py` to compare it with the Go services.

This demonstrates the difference between the **manual SDK approach** used by the Go services and the **zero-code auto-instrumentation** provided by the OpenTelemetry Operator.

[options="header",cols="30,35,35"]
|===
|Service |Language |Instrumentation method
|frontend |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|backend |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|database |Go |Manual SDK (`telemetry.Setup()` + `otelhttp`)
|**notifier** |**Python** |**Zero-code: OTel Operator init-container injection**
|===

=== Step 1: Observe the missing notifier span

Before enabling auto-instrumentation, generate traffic and look at a trace waterfall.

. Generate several note-creation requests:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 10); do
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/api/notes" \
    -H "Content-Type: application/json" \
    -d "{\"title\":\"Auto-instrumentation test $i\",\"content\":\"Exercise 6\"}"
  sleep 1
done
----

. Open **Observe** → **Traces** in the OpenShift console, set namespace to `{user}-observability-demo`, and click a recent trace.
+
The waterfall will show three hops: `frontend → backend → database`. The backend called notifier, but no notifier span appears because the Python process emits no telemetry without an agent.

=== Step 2: Inspect the pre-deployed Instrumentation CR

The OpenTelemetry Operator uses an `Instrumentation` CR as a template for agent injection. A shared CR is pre-deployed in the `observability-demo` namespace.

. Inspect it:
+
[source,bash]
----
oc get instrumentation demo-instrumentation \
  -n observability-demo -o yaml
----
+
Key fields:
+
[source,yaml]
----
spec:
  exporter:
    endpoint: http://localhost:4318   # <1>
  propagators:
    - tracecontext
    - baggage                          # <2>
  sampler:
    type: parentbased_traceidratio
    argument: "1.0"                    # <3>
  python:
    env:
      - name: OTEL_EXPORTER_OTLP_PROTOCOL
        value: http/protobuf            # <4>
----
+
<1> The auto-instrumented process sends telemetry to the sidecar on `localhost:4318`.
<2> W3C Trace Context propagators ensure the incoming `traceparent` header from the backend is read, linking the notifier span to the existing trace.
<3> 100% sampling is appropriate for this workshop environment.
<4> Forces the Python SDK to use HTTP/protobuf rather than gRPC, matching the sidecar receiver.


=== Step 3: Annotate the notifier deployment

A single `oc patch` command adds the two annotations that trigger both sidecar injection (same as the Go services) and Python agent injection (new for notifier).

. Patch the notifier deployment:
+
[source,bash]
----
oc patch deployment notifier \
  -n {user}-observability-demo \
  --type=json \
  -p='[
    {
      "op": "add",
      "path": "/spec/template/metadata/annotations",
      "value": {
        "sidecar.opentelemetry.io/inject": "sidecar",
        "instrumentation.opentelemetry.io/inject-python": "observability-demo/demo-instrumentation"
      }
    },
    {
      "op": "add",
      "path": "/spec/template/spec/serviceAccountName",
      "value": "otel-collector-sidecar"
    }
  ]'
----
+
The annotation value `observability-demo/demo-instrumentation` references the Instrumentation CR in a different namespace using the `<namespace>/<name>` format.
+
When this annotated pod is scheduled, the OpenTelemetry Operator's mutating admission webhook injects an init container that downloads `opentelemetry-distro` and `opentelemetry-instrumentation-fastapi` into a shared volume. The Python process picks them up via `PYTHONPATH` and a configurator hook—no code changes or image rebuilds required.

. Watch the rollout:
+
[source,bash]
----
oc rollout status deployment/notifier -n {user}-observability-demo
----

. Confirm the pod now has two containers (application + sidecar):
+
[source,bash]
----
oc get pods -n {user}-observability-demo \
  -l app=notifier \
  -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.containers[*].name'
----
+
.Expected output
[source,text]
----
NAME                        CONTAINERS
notifier-xxxxx              notifier, otc-container
----

. Verify the Python SDK is active by checking notifier logs:
+
[source,bash]
----
oc logs -n {user}-observability-demo \
  -l app=notifier -c notifier | head -20
----
+
Look for OpenTelemetry bootstrap messages such as `Instrumenting FastAPI` or `OpenTelemetry SDK configured`.

=== Step 4: Generate traffic and observe the 4-hop trace

. Send another batch of note-creation requests:
+
[source,bash]
----
FRONTEND_URL=$(oc get route frontend \
  -n {user}-observability-demo \
  -o jsonpath='{.spec.host}')

for i in $(seq 1 15); do
  curl -sk -o /dev/null -X POST "https://$FRONTEND_URL/api/notes" \
    -H "Content-Type: application/json" \
    -d "{\"title\":\"Traced note $i\",\"content\":\"With notifier\"}"
  sleep 1
done
echo "Done"
----

. Return to **Observe** → **Traces** in the console. A note-creation trace will now show a fourth hop:
+
[source,text]
----
frontend: POST /api/notes                     [=====================================] 130ms
  backend: POST /api/notes                    [==============================] 105ms
    database: POST /notes                     [================] 50ms
    notifier: POST /notify                    [==========] 30ms
      notifier: POST /notify → database       [======] 18ms
----
+
The `notifier` spans are produced entirely by `opentelemetry-instrumentation-fastapi` and `opentelemetry-instrumentation-httpx`—**no code was changed in `app.py`**.

. Click on the `notifier: POST /notify` span and inspect its attributes:
+
* `http.method`, `http.status_code`, `http.route` — added by FastAPI auto-instrumentation
* `k8s.pod.name`, `k8s.deployment.name` — added by the sidecar `k8sattributes` processor
* `service.name: notifier` — set via the `OTEL_SERVICE_NAME` env var injected by the operator

. Notice that the `traceparent` context passed from backend to notifier is preserved correctly: the notifier root span appears as a **child** of the backend span, maintaining the single unified trace tree.

=== Compare: manual SDK vs auto-instrumentation

[options="header",cols="35,30,35"]
|===
|Characteristic |Go (manual SDK) |Python (auto-instrumentation)
|Code change required |Yes (`telemetry.Setup()`) |**No**
|Image rebuild required |Yes |**No**
|Activation mechanism |`OTEL_ENABLED=true` env var |Pod annotation
|Span granularity |Full control (custom spans) |Framework-level (HTTP in/out)
|Custom attributes |Easy (manual API calls) |Limited without code changes
|Best for |Apps with source access |Apps without source access or rapid onboarding
|===

Browse the two approaches side-by-side using the *Source Code* tab in the workshop application:

* `telemetry/telemetry.go` — shared OTEL SDK setup (Go)
* `backend/main.go` — manual SDK usage (Go)
* `notifier/app.py` — zero OTEL imports (Python)

=== Verify

* ✓ Notifier pod has two running containers: `notifier` and `otc-container`
* ✓ Notifier logs show OpenTelemetry SDK bootstrap messages
* ✓ Traces for note creation show four service hops: `frontend → backend → notifier → database`
* ✓ Notifier spans are children of the backend span (W3C Trace Context propagation works)
* ✓ `k8s.pod.name` and `k8s.deployment.name` attributes are present on notifier spans
* ✓ `app.py` was not modified at any point in this exercise

**What you learned**: The `Instrumentation` CR is a namespace-level template. A single annotation—`instrumentation.opentelemetry.io/inject-python`—causes the OpenTelemetry Operator's admission webhook to inject an init-container that downloads and configures the Python agent at pod start. No source changes, no image rebuilds, no SDK imports. The W3C Trace Context standard ensures the notifier's spans slot directly into the existing trace tree built by the Go services.

=== Troubleshooting

**Issue**: Notifier pod shows only one container after rollout

**Solution**:
. Verify the annotation is set: `oc get deployment notifier -n {user}-observability-demo -o jsonpath='{.spec.template.metadata.annotations}'`
. Confirm the sidecar CR exists: `oc get opentelemetrycollector sidecar -n {user}-observability-demo`
. Check operator logs: `oc logs -n openshift-operators -l app.kubernetes.io/name=opentelemetry-operator | grep -i "notifier\|inject\|python"`

**Issue**: Notifier spans appear but are disconnected from the main trace

**Solution**:
. Verify `propagators` includes `tracecontext`: `oc get instrumentation demo-instrumentation -n observability-demo -o jsonpath='{.spec.propagators}'`
. Confirm `OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf` is injected: `oc exec -n {user}-observability-demo deployment/notifier -- env | grep OTEL`

**Issue**: No traces appear for the notifier after annotation

**Solution**:
. Check if the Python bootstrap ran: `oc logs -n {user}-observability-demo deployment/notifier -c notifier | grep -i opentelemetry`
. Verify the sidecar received data: `oc logs -n {user}-observability-demo deployment/notifier -c otc-container | grep -i "span\|error"`
. Ensure the Instrumentation CR is referenced with the namespace prefix: `observability-demo/demo-instrumentation`

== Learning outcomes

By completing this module, you should now understand:

* ✓ The **sidecar-to-central** collector pattern decouples application-facing collection from backend-facing export
* ✓ A **sidecar OpenTelemetryCollector** in `sidecar` mode injects a container into annotated pods without any image changes
* ✓ The **`k8sattributes` and `resourcedetection` processors** automatically enrich telemetry with Kubernetes and infrastructure metadata
* ✓ Applying `enable-otel.yaml` simultaneously activates the Go SDK, sets the OTLP endpoint, and triggers sidecar injection
* ✓ The **`spanmetrics` connector** in the central collector generates RED metrics from traces, eliminating the need for a Prometheus client library
* ✓ **Auto-instrumentation** via the `Instrumentation` CR enables zero-code telemetry for Java, Python, and Node.js applications
* ✓ A **Python/FastAPI service** (notifier) can produce full traces—including W3C context propagation—with zero source-code changes

**Business impact**: You now have a complete three-signal observability pipeline for all four microservices. A single request to your application automatically produces:

* A **distributed trace** showing the exact call path and per-service timing
* **RED metrics** (rate, errors, duration) queryable in Prometheus—without any metrics code in the application
* **Structured log records** correlated to the trace via trace ID—without any manual log attribute configuration

This gives the platform engineering team full observability with minimal developer friction.

== Module summary

You activated the full OpenTelemetry telemetry pipeline for the workshop application—from sidecar collector creation through to live trace visualization in the OpenShift console.

**What you accomplished**:

* Verified the OpenTelemetry Operator and the pre-deployed central collector in `observability-demo`
* Created a sidecar `OpenTelemetryCollector` CR in your namespace with a three-signal pipeline (traces, metrics, logs)
* Applied `enable-otel.yaml` to activate the Go SDK and trigger sidecar injection on all three Go deployments
* Analyzed live traces in **Observe → Traces**, identified span-level bottlenecks, and correlated spans with log records
* Explored the central collector's `spanmetrics` connector and queried auto-generated RED metrics in Prometheus
* Enabled zero-code Python auto-instrumentation on the **notifier** service using a single pod annotation and the `Instrumentation` CR
* Observed a **four-hop trace** (`frontend → backend → notifier → database`) with full context propagation across Go and Python services

**Key concepts mastered**:

* **Sidecar mode**: The operator injects the collector as a second container into annotated pods
* **k8sattributes and resourcedetection**: Processors that attach Kubernetes context metadata to all telemetry
* **Two-tier architecture**: Sidecar handles application-facing collection; central collector handles backend-facing export and routing
* **spanmetrics connector**: Generates Prometheus-queryable RED metrics automatically from trace spans
* **Auto-instrumentation**: `Instrumentation` CR + pod annotation enables agent injection for Java, Python, Node.js, and .NET without code changes
* **Cross-language tracing**: W3C Trace Context headers propagate correctly between Go (`otelhttp`) and Python (`opentelemetry-instrumentation-httpx`), forming a single unified trace tree

You have now completed the core modules of the OpenShift Observability Workshop. Continue to the conclusion to review key takeaways and next steps.
