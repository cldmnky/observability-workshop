= Module 3: Distributed tracing
:source-highlighter: rouge
:toc: macro
:toclevels: 1

With metrics from Module 1 and centralized logging from Module 2, you can now detect issues and understand what went wrong. However, a challenge remains: when your application experiences slow response times, metrics show overall slowness and logs show individual service behavior, but neither reveals _where in the call chain_ the delay originates.

Your organization's application—the frontend, backend, and database services you deployed in Module 2—processes every user request as a chain of HTTP calls across three distinct services. When performance degrades, you need to see the complete path of each request, including the time spent in each hop.

In this module, you'll explore distributed tracing with Tempo, enable the Distributed Tracing UI plugin in the OpenShift console, and examine how the Go application is instrumented with the OpenTelemetry SDK. Traces will not yet appear until Module 4 activates the telemetry pipeline—but by the end of this module you will understand exactly how they flow.

toc::[]

== Learning objectives

By the end of this module, you'll be able to:

* Understand distributed tracing concepts (spans, traces, context propagation)
* Deploy and configure Tempo for trace storage and visualization
* Instrument applications to emit trace data using the OpenTelemetry SDK
* Enable and navigate the Distributed Tracing UI plugin in the OpenShift console
* Understand the relationship between traces, metrics, and logs for incident investigation

== Understanding distributed tracing

Before enabling tracing, you need to understand how distributed tracing works in microservice architectures.

=== Core tracing concepts

**Trace**: The complete journey of a single request through your system.

* Example: A user submits a note → frontend service → backend service → database service
* A trace captures the entire chain, start to finish, with timing and status at each step.

**Span**: One unit of work within a trace.

* Example: `backend: POST /api/notes` — one hop in the note-creation trace
* Contains: operation name, start time, duration, HTTP status code, and key-value attributes

**Context propagation**: The mechanism that ties spans together across service boundaries.

* Each service receives an incoming trace context via the `traceparent` HTTP header
* It creates a child span linked to the caller's span, then propagates the context further downstream
* Uses the W3C Trace Context standard (`traceparent`, `tracestate` headers)

**Parent-child relationships**: Spans form a tree.

* Root span: `frontend: GET /new-note`
* Child span: `backend: POST /api/notes` (called by frontend)
* Grandchild span: `database: POST /api/events` (called by backend)

=== Why distributed tracing matters

**Without tracing**: You see symptoms, not causes.

* Metrics show: 95th-percentile API response time increased from 80ms to 950ms
* Logs show: all three services logged warnings at the same time
* **Question**: Which service is actually slow?

**With tracing**: You see the complete picture.

* Trace shows: frontend→backend took 20ms; backend→database took 880ms (bottleneck found)
* Other services operated normally
* **Answer**: The database service query needs optimization

=== Tempo architecture

**Tempo** is a distributed tracing backend optimized for storing and querying traces on cost-effective object storage.

**Key components**:

* **Distributor**: Receives trace data from instrumented applications over OTLP (gRPC port 4317, HTTP port 4318)
* **Ingester**: Buffers spans in memory and writes them to object storage
* **Querier**: Serves trace queries by reading from both the ingester cache and object storage in parallel
* **Query Frontend**: Load-balances query traffic across Querier pods
* **Compactor**: Merges and optimizes stored trace blocks over time

**Storage**: Tempo requires S3-compatible object storage. In this workshop the TempoStack uses in-cluster object storage provisioned by the OpenShift Data Foundation (ODF) NooBaa Multi-Cloud Gateway.

**Integration**: Tempo is queried by the Distributed Tracing UI plugin built into the OpenShift console via the Cluster Observability Operator. There is no separate Jaeger pod—the query interface is embedded directly in the console.

== Exercise 1: Verify Tempo deployment

The Tempo distributed tracing stack was deployed via GitOps as part of the workshop infrastructure. Verify that all components are running and ready.

=== Steps

. Verify the Tempo Operator is running:
+
[source,bash]
----
oc get pods -n openshift-tempo-operator
----
+
.Expected output
[source,text]
----
NAME                                          READY   STATUS    RESTARTS   AGE
tempo-operator-controller-manager-xxxxx       2/2     Running   0          1h
----

. Check the TempoStack instance:
+
[source,bash]
----
oc get tempostack -n openshift-tempo-operator
----
+
.Expected output
[source,text]
----
NAME    AGE   CONDITION
tempo   1h    Ready
----
+
The `CONDITION` must be **Ready** before traces can be ingested.

. Verify each Tempo component is running:
+
[source,bash]
----
oc get pods -n openshift-tempo-operator -l app.kubernetes.io/instance=tempo
----
+
.Expected output
[source,text]
----
NAME                                        READY   STATUS    RESTARTS   AGE
tempo-tempo-compactor-xxxxx                 1/1     Running   0          1h
tempo-tempo-distributor-xxxxx               1/1     Running   0          1h
tempo-tempo-ingester-0                      1/1     Running   0          1h
tempo-tempo-querier-xxxxx                   1/1     Running   0          1h
tempo-tempo-query-frontend-xxxxx            1/1     Running   0          1h
----

. Confirm the distributor service endpoint (used by the OpenTelemetry Collector to write traces):
+
[source,bash]
----
oc get svc tempo-tempo-distributor -n openshift-tempo-operator
----
+
.Expected output
[source,text]
----
NAME                      TYPE        CLUSTER-IP      PORT(S)
tempo-tempo-distributor   ClusterIP   172.30.x.x      4317/TCP, 4318/TCP
----
+
Port **4317** is OTLP gRPC and port **4318** is OTLP HTTP. The central OpenTelemetry Collector forwards traces to this endpoint.

. Query the Tempo status API to confirm readiness:
+
[source,bash]
----
oc exec -n openshift-tempo-operator \
  deployment/tempo-tempo-query-frontend -- \
  wget -qO- http://localhost:3200/ready
----
+
.Expected output
[source,text]
----
ready
----

=== Verify

Check that your tracing infrastructure is operational:

* ✓ Tempo Operator pod is Running (2/2 containers)
* ✓ TempoStack instance condition is Ready
* ✓ All Tempo component pods (distributor, ingester, querier, query-frontend, compactor) are Running
* ✓ Tempo distributor service exposes ports 4317 and 4318
* ✓ Query frontend responds with `ready`

**What you learned**: The TempoStack operator deploys and manages all Tempo components. The distributor is the write endpoint; the query-frontend is the read endpoint used by the OpenShift console UI plugin.

=== Troubleshooting

**Issue**: TempoStack condition shows `Degraded` or `Progressing`

**Solution**:
. Describe the TempoStack for condition details: `oc describe tempostack tempo -n openshift-tempo-operator`
. Check component pod status: `oc get pods -n openshift-tempo-operator -l app.kubernetes.io/instance=tempo`
. Verify object storage: `oc get pvc -n openshift-tempo-operator`
. Check operator logs: `oc logs -n openshift-tempo-operator -l control-plane=controller-manager -c manager`

**Issue**: Distributor pod is CrashLoopBackOff

**Solution**:
. View distributor logs: `oc logs -n openshift-tempo-operator -l app.kubernetes.io/component=distributor`
. Verify the object storage secret exists: `oc get secret -n openshift-tempo-operator | grep tempo`

== Exercise 2: Enable the Distributed Tracing UI plugin

The Cluster Observability Operator ships a Distributed Tracing UI plugin that adds an **Observe → Traces** page to the OpenShift console. You register it by creating a `UIPlugin` custom resource.

=== Steps

. Verify the Cluster Observability Operator is running:
+
[source,bash]
----
oc get pods -n openshift-cluster-observability-operator
----
+
.Expected output
[source,text]
----
NAME                                                          READY   STATUS    RESTARTS   AGE
cluster-observability-operator-controller-manager-xxxxx       2/2     Running   0          1h
----

. Check whether the Distributed Tracing UIPlugin already exists:
+
[source,bash]
----
oc get uiplugin distributed-tracing 2>/dev/null && echo "exists" || echo "not found"
----
+
If the output is `exists`, skip to step 4.
+
If the output is `not found`, create it in the next step.

. Create the Distributed Tracing UIPlugin:
+
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: distributed-tracing
spec:
  type: DistributedTracing
EOF
----
+
The Cluster Observability Operator detects this CR and configures the OpenShift console to load the distributed tracing frontend plugin, connecting it automatically to the TempoStack instance in the cluster.
+
[NOTE]
====
The `UIPlugin` CR links the console plugin to the TempoStack query-frontend based on available `TempoStack` resources in the cluster. No manual URL configuration is required.
====

. Wait for the plugin to become available:
+
[source,bash]
----
oc get uiplugin distributed-tracing -w
----
+
.Expected output
[source,text]
----
NAME                  TYPE                AGE   AVAILABLE
distributed-tracing   DistributedTracing  30s   true
----
+
Press Ctrl+C when `AVAILABLE` shows `true`.

. Reload the OpenShift console in your browser.
+
Navigate to **Observe** → **Traces** in the left-hand navigation.
+
You should see a traces search page with:
+
* A **Namespace** filter
* A **Service** filter
* A scatter plot area (currently empty—no traces have been emitted yet)
* A trace list area (also empty—this is expected)

=== Verify

Check that the UI plugin is operational:

* ✓ UIPlugin resource exists with `AVAILABLE: true`
* ✓ **Observe → Traces** appears in the OpenShift console navigation
* ✓ Namespace and Service filter dropdowns are present
* ✓ The page loads without error (no traces visible yet—expected)

**What you learned**: The Distributed Tracing UI plugin is registered as a `UIPlugin` CR managed by the Cluster Observability Operator. The plugin connects to the TempoStack query-frontend and renders the trace search UI directly in the OpenShift console—no separate Jaeger deployment is required. For more information see the link:https://docs.redhat.com/en/documentation/red_hat_openshift_cluster_observability_operator/1-latest/html/ui_plugins_for_red_hat_openshift_cluster_observability_operator/distributed-tracing-ui-plugin[Distributed Tracing UI plugin documentation^].

=== Troubleshooting

**Issue**: UIPlugin stays at `AVAILABLE: false`

**Solution**:
. Describe the UIPlugin: `oc describe uiplugin distributed-tracing`
. Check COO controller logs: `oc logs -n openshift-cluster-observability-operator -l control-plane=controller-manager`
. Verify the TempoStack is in Ready state (the plugin requires a Ready TempoStack to configure its backend URL)

**Issue**: **Observe → Traces** does not appear after browser reload

**Solution**:
. Hard-refresh the browser (Ctrl+Shift+R or Cmd+Shift+R)
. Confirm `oc get uiplugin distributed-tracing` shows `AVAILABLE: true`
. Verify you have console access with the appropriate role

== Exercise 3: Explore the Observe → Traces interface

Before traces flow, familiarize yourself with the trace search UI so you know exactly what to look for when Module 4 activates telemetry.

=== Steps

. In the OpenShift console navigate to **Observe** → **Traces**.

. Set the namespace filter:
+
In the **Namespace** dropdown, select `{user}-observability-demo`.
+
This scopes the search to the namespace where your application runs.

. Explore the time range selector:
+
The default window is the last 30 minutes. You can expand it to 1 hour or 6 hours.
+
In Module 4, after enabling telemetry, you will change this to **Last 5 minutes** to see fresh traces immediately.

. Review the scatter plot:
+
When traces are present, each point represents a single trace:
+
* X-axis: trace start time
* Y-axis: total trace duration (ms)
* Bubble size: number of spans in the trace
+
Clusters of points at the top of the chart indicate slow traces. Isolated high points are outliers worth investigating.

. Review the trace list columns:
+
When traces appear, the list shows:
+
* **Trace name**: root span operation (for example, `frontend: GET /new-note`)
* **Spans**: total number of spans in the trace
* **Duration**: end-to-end time
* **Start time**: when the root span began

. Explore the **Show query** toggle:
+
Click **Show query** beneath the filter bar.
+
This reveals a raw **TraceQL** query field. TraceQL is the Tempo query language, similar to PromQL for Prometheus or LogQL for Loki.
+
Find all traces from the frontend service:
+
[source,text]
----
{ resource.service.name = "frontend" }
----
+
Filter by duration:
+
[source,text]
----
{ resource.service.name = "backend" && duration > 200ms }
----
+
Find all errored traces:
+
[source,text]
----
{ status = error }
----

=== Verify

* ✓ **Observe → Traces** page loads cleanly for namespace `{user}-observability-demo`
* ✓ Time range selector is functional
* ✓ Scatter plot and trace list areas are present (empty is expected at this stage)
* ✓ **Show query** toggle reveals the TraceQL editor
* ✓ Basic TraceQL syntax is understood

**What you learned**: The Distributed Tracing UI integrates directly into the OpenShift console. TraceQL lets you filter traces by service, operation name, duration, error status, and any custom span attribute—the same way PromQL queries metrics and LogQL queries logs.

== Exercise 4: Explore the application's OpenTelemetry instrumentation

The three Go services—frontend, backend, and database—are already instrumented with the OpenTelemetry SDK. Telemetry generation is gated by an environment variable so it can be enabled without rebuilding the container image.

=== Steps

. Inspect the shared telemetry package:
+
The repository contains a shared `telemetry` package used by all three services at `src/telemetry/telemetry.go`.
+
TIP: Open the *Source Code* tab in the workshop application (the running frontend) and navigate to `telemetry/telemetry.go` to browse the file with syntax highlighting.
+
Key points in this package:
+
* **`Enabled()` function**: Returns `true` only when `OTEL_ENABLED=true` is set in the environment. All SDK initialization is skipped when false, so the application behaves identically to an uninstrumented binary.
* **`Setup()` function**: Initializes three OTLP HTTP exporters when enabled—trace, metric, and log—all targeting `OTEL_EXPORTER_OTLP_ENDPOINT`. In Module 4, this will be `http://localhost:4318` (the injected sidecar collector).

. Inspect the backend service instrumentation:
+
TIP: Open the *Source Code* tab in the workshop application and select `backend/main.go` to browse the file directly.
+
Or run the grep from the workshop terminal to scan for instrumentation lines:
+
[source,bash]
----
grep -n "otelhttp\|otelslog\|telemetry\." src/backend/main.go | head -20
----
+
You will see three instrumentation layers activated when `OTEL_ENABLED=true`:
+
[options="header",cols="30,70"]
|===
|Layer |Purpose
|`telemetry.Setup()` |Creates the global TraceProvider, MeterProvider, and LoggerProvider from the OTLP exporters
|`otelslog.NewHandler()` |Bridges the Go standard `slog` logger to the OTel log exporter—every structured log line is emitted as an OTel log record carrying the active trace ID
|`otelhttp.NewTransport()` |Wraps the outbound HTTP client so the W3C `traceparent` header is injected into every downstream call
|===

. Understand the server-side instrumentation:
+
The inbound HTTP handler is wrapped with `otelhttp.NewHandler()`, which:
+
* Creates a server span for every inbound request
* Extracts the incoming `traceparent` header and registers this span as a child of the calling service's span
* Automatically records HTTP attributes (`http.method`, `http.route`, `http.status_code`) on the span

. Understand context propagation across services:
+
The trace context flows automatically through your microservices:
+
[source,text]
----
Browser
  +-- frontend (otelhttp server span: GET /new-note)
       |  injects traceparent into outbound request
       +-- backend (otelhttp server span: POST /api/notes)
            |  injects traceparent into outbound request
            +-- database (otelhttp server span: POST /api/events)
----
+
Because every service uses `otelhttp` for both inbound (handler) and outbound (transport) HTTP calls, the trace ID and parent span ID are automatically threaded through the entire call chain with no manual span creation required in business logic code.

. Review the `enable-otel.yaml` patch file:
+
TIP: Open the *Source Code* tab in the workshop application and select `enable-otel.yaml` to browse the full patch file.
+
Or view it in the terminal:
+
[source,bash]
----
cat src/enable-otel.yaml
----
+
This file contains three `Deployment` patches—one per service. When applied, each patch:
+
* Sets `OTEL_ENABLED=true` to activate the SDK
* Sets `OTEL_SERVICE_NAME` to the service name (becomes the `service.name` resource attribute)
* Sets `OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318` to send telemetry to the injected sidecar
* Adds the pod annotation `sidecar.opentelemetry.io/inject: "sidecar"` to trigger sidecar injection
* Sets `serviceAccountName: otel-collector-sidecar` for RBAC access to the Kubernetes API

. Verify the current state of the deployments:
+
[source,bash]
----
oc get deployment frontend backend database \
  -n {user}-observability-demo \
  -o custom-columns='NAME:.metadata.name,CONTAINERS:.spec.template.spec.containers[*].name'
----
+
.Expected output
[source,text]
----
NAME       CONTAINERS
frontend   frontend
backend    backend
database   database
----
+
Each deployment currently has a single application container. After applying `enable-otel.yaml` in Module 4, each pod will gain a second container—the injected sidecar collector.

=== Verify

* ✓ `src/telemetry/telemetry.go` gates all SDK initialization on `OTEL_ENABLED=true`
* ✓ `otelhttp.NewHandler()` creates server spans and extracts incoming trace context
* ✓ `otelhttp.NewTransport()` injects the `traceparent` header into all outbound HTTP calls
* ✓ `otelslog.NewHandler()` bridges Go structured logs to OTel log records
* ✓ `enable-otel.yaml` contains the sidecar annotation and environment variable patches for all three services
* ✓ Current deployments have one container each (no OTel yet)

**What you learned**: Effective OpenTelemetry Go instrumentation uses three layers—trace provider, `otelhttp` middleware, and log bridge—to emit traces, metrics, and correlated logs from a single SDK setup call. The `otelhttp` transport ensures W3C trace context propagation across all service boundaries automatically, without any manual span creation in application code.

== Learning outcomes

By completing this module, you should now understand:

* ✓ A **trace** is the complete journey of a request; a **span** is one operation within that trace
* ✓ **Context propagation** via W3C `traceparent` headers links spans across service boundaries automatically
* ✓ **Tempo** stores traces on object storage with distinct distributor (write) and query-frontend (read) components
* ✓ The **Distributed Tracing UI plugin** is registered as a `UIPlugin` CR and adds **Observe → Traces** to the OpenShift console
* ✓ **TraceQL** filters traces by service name, operation, duration, status, and any span attribute
* ✓ The Go application uses `otelhttp` and `otelslog` for automatic instrumentation, gated by `OTEL_ENABLED=true`

**What comes next**: Module 4 activates the full telemetry pipeline by deploying a sidecar OpenTelemetry Collector in your namespace, patching the deployments with `enable-otel.yaml`, and configuring the collector to fan out telemetry to Tempo, Prometheus, and Loki. The **Observe → Traces** page you explored in this module will then show live trace data from your own application.

== Module summary

You verified the Tempo distributed tracing infrastructure, enabled the Distributed Tracing UI plugin in the OpenShift console, familiarized yourself with the trace search UI, and examined the OpenTelemetry SDK instrumentation built into the workshop application.

**Key concepts mastered**:

* **Trace and span**: A trace is a tree of spans; each span represents one service operation with timing and attributes
* **Context propagation**: `traceparent` HTTP headers link spans across service boundaries using the W3C standard
* **TempoStack**: Operator-managed, with distributor, ingester, querier, query-frontend, and compactor components
* **UIPlugin CR**: Registers the Distributed Tracing console plugin via the Cluster Observability Operator
* **TraceQL**: Query language for filtering traces by service, duration, status, and span attributes
* **SDK gating**: `OTEL_ENABLED=true` activates all telemetry exporters; the application is silent without it

Continue to Module 4 to activate the telemetry pipeline and observe live traces from your own microservices.
